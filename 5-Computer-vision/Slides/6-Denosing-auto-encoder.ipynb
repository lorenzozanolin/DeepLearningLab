{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8337b007",
   "metadata": {},
   "source": [
    "# Denoising Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2df615",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ingambe/denoising_auto_encoder_class/runs/3t4o0ha8\" target=\"_blank\">olive-aardvark-8</a></strong> to <a href=\"https://wandb.ai/ingambe/denoising_auto_encoder_class\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ingambe/denoising_auto_encoder_class/runs/3t4o0ha8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x16a031a60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import utils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import wandb\n",
    "wandb.init(project='denoising_auto_encoder_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721c4dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use Auto Encoder to reconstruct a partially destructed image\n",
    "\n",
    "The model will map the noisy input image to the latent space and back to the input space while removing the noise\n",
    "\n",
    "<center>\n",
    "    <img src='images/15_denoising_ae.png' width=55% style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-3/'>NYU Deep Learning</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d09f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bellow is a plot of the traveling distance in a denoising auto encoder\n",
    "\n",
    "<center>\n",
    "    <img src='images/17_distance.png' width=55% style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-3/'>NYU Deep Learning</a></p>\n",
    "</center>\n",
    "\n",
    "The lighter the colour, the longer the distance a point travelled. From the diagram, we can tell that the points at the corners travelled close to 1 unit, whereas the points within the 2 branches didnâ€™t move at all since they are attracted by the top and bottom branches during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d434798",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.view(x.size(0), 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edc1cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def layer_init(m):\n",
    "    torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "    torch.nn.init.constant_(m.bias, 0)\n",
    "    return m\n",
    "\n",
    "class CNNDenosingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(1, 32, 5)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 32, 4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 3)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3)),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            layer_init(nn.ConvTranspose2d(64, 64, 3)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.ConvTranspose2d(64, 32, 3)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.ConvTranspose2d(32, 32, 4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.ConvTranspose2d(32, 1, 5))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a640def",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "indices = torch.arange(5000)\n",
    "mnist_5k = torch.utils.data.Subset(dataset, indices)\n",
    "dataloader = DataLoader(mnist_5k, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b22e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNDenosingAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af0d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15d16ad2dee424c991db4310f7e051a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "noise = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    losses = 0\n",
    "    for img, label in tqdm(dataloader, unit='batch'):\n",
    "        with torch.no_grad():\n",
    "            noisy_imgs = noise(img)\n",
    "        output = model(noisy_imgs)\n",
    "        loss = criterion(output, img)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    wandb.log({'loss': losses / len(dataloader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d58069",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pic = to_img(noisy_imgs.cpu().data)\n",
    "plt.figure(figsize=(18, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(in_pic[i+4])\n",
    "    plt.axis('off')\n",
    "    \n",
    "out_pic = to_img(output.cpu().data)\n",
    "plt.figure(figsize=(18, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(out_pic[i+4])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc53b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
