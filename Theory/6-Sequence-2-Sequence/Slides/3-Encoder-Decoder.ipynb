{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d2c158",
   "metadata": {},
   "source": [
    "# Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04b74d8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ingambe/encoder_decoder/runs/wgmo97qq\" target=\"_blank\">snowy-glade-2</a></strong> to <a href=\"https://wandb.ai/ingambe/encoder_decoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ingambe/encoder_decoder/runs/wgmo97qq?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x16307d190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import wandb\n",
    "wandb.init(project='encoder_decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39bdc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a lot of sequence to sequence problem such as translation, you can't simply translate word by word the input to get a correct output  \n",
    "You have to first read the full original sentence to get a context before translation  \n",
    "Also, the input sentence and the output sequence are of variable length, so they can't be processed sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d492916b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To handle such kind of problem, we need an architecture to first get the global context before starting producing output called an *encoder-decoder architecture*\n",
    "\n",
    "The first component is an *encoder*, it takes a variable-length sequence as the input and transforms it into a state with a fixed shape representing the context  \n",
    "The second component is a *decoder*, it maps the encoded state of a fixed shape to a variable-length sequence  \n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src='data/seq2seq.svg' width=\"65%\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='d2l.ai'>D2L</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f3f10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We use an embedding layer to obtain the feature vector for each token in the input sequence. \n",
    "The weight of an embedding layer is a matrix whose number of rows equals to the size of the input vocabulary (`vocab_size`) and number of columns equals to the feature vectorâ€™s dimension (`embed_size`). \n",
    "For any input token index $i$, the embedding layer fetches the $i^{\\mathrm{th}}$ row (starting from $0$) of the weight matrix to return its feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb083fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Embedding(3, 4)\n",
    "\n",
    "# 0 -> [0.3, 0.5, 0.12, 0.4]\n",
    "# 1 -> [0.23, 0.3, 0.22, 0.42]\n",
    "# 2 -> [0.03, 0.12, 0.11, 0.65]\n",
    "\n",
    "# cat 0 => [1, 0, 0]\n",
    "# dog 1 => [0, 1, 0]\n",
    "# car 2 => [0, 0, 1]\n",
    "\n",
    "# BEFORE:\n",
    "# teaching: [0, 0, 0, 0, 0, 1, ...., 0, ..., 0] # \\30 000\\\n",
    "\n",
    "nn.Embedding(30000, 256)\n",
    "\n",
    "# teaching: [0.1, 0.24, -0.3, ..., 0.9, 0.88] # \\256\\\n",
    "\n",
    "# Tokenize:\n",
    "# word_a, word_b, word_c\n",
    "# 0       1       2\n",
    "\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
    "        X = self.embedding(X)\n",
    "        # In RNN models, the first axis corresponds to time steps\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # When state is not mentioned, it defaults to zeros\n",
    "        output, state = self.rnn(X)\n",
    "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
    "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7230801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba5c799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # Broadcast `context` so it has the same `num_steps` as `X`\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81806845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0442fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X):\n",
    "        enc_outputs = self.encoder(enc_X)\n",
    "        dec_state = self.decoder.init_state(enc_outputs)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4dc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d2b40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we pad the sequence as the beginning, we need to not consider this in the `CrossEntropy`loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bb520f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    \n",
    "    def forward(self, pred, label, valid_len):\n",
    "        \n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len).float()\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82785112",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Download the machine translation dataset, it contains English sentences translated to French  \n",
    "Each input/output have a fixed length, specified by `num_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ba725",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "\n",
    "batch_size, num_steps = 64, 10\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "lr, num_epochs = 0.005, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d400eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "net.apply(xavier_init_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "net.train()\n",
    "\n",
    "cum_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        X, X_valid_len, Y, Y_valid_len = [x for x in batch]\n",
    "        bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0]).reshape(-1, 1)\n",
    "        dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "        Y_hat, _ = net(X, dec_input)\n",
    "        l = loss(Y_hat, Y, Y_valid_len)\n",
    "        l.sum().backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            wandb.log({\n",
    "            'loss': l.sum().item()\n",
    "            })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5eda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
