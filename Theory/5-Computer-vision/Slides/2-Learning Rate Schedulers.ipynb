{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469c7596",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Learning Rate Scheduler\n",
    "\n",
    "adjusting the rate of the weight update throught the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1a623a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5cfad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The magnitude of the learning rate is important  \n",
    "A too small learning rate can slow down convergence often leading to suboptimal result  \n",
    "On the other hand, a too large learning rate can makes the optimization diverge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c64ed1",
   "metadata": {},
   "source": [
    "At the beginning of the training, the error is large so we can afford a higher learning rate  \n",
    "Toward the end of the training, the learning rate needs to be reduced to avoid boucing around the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773461e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can access and modify the learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f44217",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ded53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer.param_groups[0][\"lr\"] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369725b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, it is not practical to modify it by hand  \n",
    "Fortunately, Pytorch defines by default a certain number of scheduler: https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a267c0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A rudimentary approach is to halve by a certain factor the learning rate at specific epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e0264e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 0], gamma=0.1)\n",
    "for epoch in range(epochs):\n",
    "    #train(...)\n",
    "    #validate(...)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72dcec4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The problem is that we rather have a smoother reduction of the learning rate rather than suddent drop  \n",
    "A very common and empirically effective scheduler is the cosine scheduler\n",
    "\n",
    "<center>\n",
    "    <img src='images/cosine.png' width='60%'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bed91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 0.0001\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=min_lr)\n",
    "for epoch in range(epochs):\n",
    "    #train(...)\n",
    "    #validate(...)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83878c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In certain special case, there is a need to \"warm-up\" the learning rate  \n",
    "For example, if you have strong features than can dominate other, having a too high learning rate at the beginning can cause the neural network to only focus on these features and overfit very fast\n",
    "\n",
    "<center>\n",
    "    <img src='images/warmup.png' width='80%'/>\n",
    "    <p>Source: <a href='https://huggingface.co/docs/transformers/main_classes/optimizer_schedules'>Hugging Face </a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc527d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_before_restart = 20\n",
    "mult_fact_restart = 10\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, iter_before_restart, mult_fact_restart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1ad13",
   "metadata": {},
   "source": [
    "Some recent works have shown that a warm-up period is generally beneficial for very deep neural network  \n",
    "And it's crutial for certain architectures (i.e., transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d7e90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One scheduler working very well emperically is the `ReduceLROnPlateau` one  \n",
    "It reduces the learning rate by a given factor (by default $0.1$) after $X$ steps without improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7871b3c7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19e9c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As the optimizer itself contains some parameters (i.e., momentum, RMSProp) should also be saved and load to continue training with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8749cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(optimizer.state_dict(), 'optimizer.pt')\n",
    "optimizer.load_state_dict(torch.load('optimizer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16978f6",
   "metadata": {},
   "source": [
    "The scheduler state can also be saved to be loaded later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82e8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scheduler.state_dict(), 'scheduler.pt')\n",
    "scheduler.load_state_dict(torch.load('scheduler.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2964477",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To go further: https://iconof.com/1cycle-learning-rate-policy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4e8a2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
