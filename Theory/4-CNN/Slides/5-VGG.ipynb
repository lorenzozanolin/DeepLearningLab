{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14781708",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Visual Geometry Group Architecture\n",
    "\n",
    "From individual operation to block of operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e90e07",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /Users/ingambe/miniforge3/envs/deep-learning/lib/python3.9/site-packages (1.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4a9e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than stacking layers, we stack blocks that we repeat $X$ time  \n",
    "Blocks can contain a different number of convolution layers but have the same *resolution* and *stride*\n",
    "\n",
    "<center>\n",
    "    <img src='images/vgg.svg' width=55% style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='http://d2l.ai/'>D2L</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672c24e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each block is composed of convolutional layers with padding to maintain the resolution, ReLU activation function, and a MaxPooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbeb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e4201",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Original **VGG** architecture called **VGG-11** can be represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dddd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch_11 = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) # tuple (nb_conv_layers, channel_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd11e82",
   "metadata": {},
   "source": [
    "One version often used called **VGG-16** can be represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644985fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch_16 = ((2, 64), (2, 128), (3, 256), (3, 512), (3, 512)) # tuple (nb_conv_layers, channel_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9d9f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thanks to Pytorch's object-oriented philosophy, it's very easy to create blocks and assemble them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e6c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 3\n",
    "    # The convolutional part\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),\n",
    "        # The fully-connected part\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), # output resolution specific to Imagenet (224, 224)\n",
    "        nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 1000))\n",
    "\n",
    "net = vgg(conv_arch_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d36681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               --                        --\n",
       "├─Sequential: 1-1                        [32, 64, 112, 112]        --\n",
       "│    └─Conv2d: 2-1                       [32, 64, 224, 224]        1,792\n",
       "│    └─ReLU: 2-2                         [32, 64, 224, 224]        --\n",
       "│    └─MaxPool2d: 2-3                    [32, 64, 112, 112]        --\n",
       "├─Sequential: 1-2                        [32, 128, 56, 56]         --\n",
       "│    └─Conv2d: 2-4                       [32, 128, 112, 112]       73,856\n",
       "│    └─ReLU: 2-5                         [32, 128, 112, 112]       --\n",
       "│    └─MaxPool2d: 2-6                    [32, 128, 56, 56]         --\n",
       "├─Sequential: 1-3                        [32, 256, 28, 28]         --\n",
       "│    └─Conv2d: 2-7                       [32, 256, 56, 56]         295,168\n",
       "│    └─ReLU: 2-8                         [32, 256, 56, 56]         --\n",
       "│    └─Conv2d: 2-9                       [32, 256, 56, 56]         590,080\n",
       "│    └─ReLU: 2-10                        [32, 256, 56, 56]         --\n",
       "│    └─MaxPool2d: 2-11                   [32, 256, 28, 28]         --\n",
       "├─Sequential: 1-4                        [32, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-12                      [32, 512, 28, 28]         1,180,160\n",
       "│    └─ReLU: 2-13                        [32, 512, 28, 28]         --\n",
       "│    └─Conv2d: 2-14                      [32, 512, 28, 28]         2,359,808\n",
       "│    └─ReLU: 2-15                        [32, 512, 28, 28]         --\n",
       "│    └─MaxPool2d: 2-16                   [32, 512, 14, 14]         --\n",
       "├─Sequential: 1-5                        [32, 512, 7, 7]           --\n",
       "│    └─Conv2d: 2-17                      [32, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-18                        [32, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-19                      [32, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-20                        [32, 512, 14, 14]         --\n",
       "│    └─MaxPool2d: 2-21                   [32, 512, 7, 7]           --\n",
       "├─Flatten: 1-6                           [32, 25088]               --\n",
       "├─Linear: 1-7                            [32, 4096]                102,764,544\n",
       "├─ReLU: 1-8                              [32, 4096]                --\n",
       "├─Dropout: 1-9                           [32, 4096]                --\n",
       "├─Linear: 1-10                           [32, 4096]                16,781,312\n",
       "├─ReLU: 1-11                             [32, 4096]                --\n",
       "├─Dropout: 1-12                          [32, 4096]                --\n",
       "├─Linear: 1-13                           [32, 1000]                4,097,000\n",
       "==========================================================================================\n",
       "Total params: 132,863,336\n",
       "Trainable params: 132,863,336\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 243.73\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 1903.42\n",
       "Params size (MB): 531.45\n",
       "Estimated Total Size (MB): 2454.14\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(net, input_size=(32, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb1b05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In VGG paper, authors found that having narrow convolution kernel (i.e. 3x3) but deeper model was better than larger kernels and less deep CNN. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5b8c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
