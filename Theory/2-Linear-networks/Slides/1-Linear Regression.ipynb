{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f4628b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b398477",
   "metadata": {},
   "source": [
    "Modeling the relationship between one or more independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c164fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fc38f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b485c",
   "metadata": {},
   "source": [
    "Regression is usually used in prediction problem  \n",
    "**Examples**: Predicting stock prices, house prices, COVID cases, demands for specific products, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd160e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8aeb2",
   "metadata": {},
   "source": [
    "**Linear** Regression assumes that the relationship between the features $\\mathbf{x}$ and the targets $y$ is linear,\n",
    "i.e., that $y$ can be expressed as a weighted sum of the elements in $\\mathbf{x}$ plus some Gaussian observation noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720835a",
   "metadata": {},
   "source": [
    "**Example** :We wish to estimate the prices of houses based on their area and age.\n",
    "We need a set of example, called a *training set*, where each row (containing the data corresponding to one sale)\n",
    "is called an *example*  \n",
    "\n",
    "The thing we are trying to predict is called a *label*  \n",
    "The variables upon which the predictions are based are called *features*  \n",
    "\n",
    "$n$ represent the number of examples in our dataset. \n",
    "$\\mathbf{x}^{(i)}$ denotes the $i$-th sample and $x_j^{(i)}$ denotes its $j$-th coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df0e47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeceff99",
   "metadata": {},
   "source": [
    "$$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b.$$\n",
    "\n",
    "$w_{\\mathrm{area}}$ and $w_{\\mathrm{age}}$\n",
    "are called *weights*, and $b$ is called a *bias*\n",
    "\n",
    "Weights determine the influence of each feature on our prediction. The bias determines the value of the estimate when all features are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74ea52",
   "metadata": {},
   "source": [
    "This is called **linear** regression but it's not. **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712eb242",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68b33f",
   "metadata": {},
   "source": [
    "We want to learn the vector of weights $w$ that provides the best predictions $\\hat{y}$ as\n",
    "\n",
    "$$\\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375714f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let $\\mathbf{x} \\in \\mathbb{R}^d$ the vector containing all the features,\n",
    "we express our model using a dot product:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a002d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a collection of features $\\mathbf{X}$, the predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ can be expressed via the matrix-vector product:\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46489d78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metric to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee2039",
   "metadata": {},
   "source": [
    "To select the best parameters, we need to be capable to compare 2 set of weights\n",
    "\n",
    "The *loss* function quantifies the distance\n",
    "between the *real* and *predicted* value of the target.\n",
    "The loss will usually be a **non-negative number**\n",
    "where smaller values are better with an optimum of **0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5290f",
   "metadata": {},
   "source": [
    "**Squared error** $$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n",
    "\n",
    "with $\\hat{y}^{(i)}$ our prediction and $y^{(i)}$ the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405af50e",
   "metadata": {},
   "source": [
    "To mesure the loss on the entire dataset, we simply average the loss of each item\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a021bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6059bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to train this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7b7bc",
   "metadata": {},
   "source": [
    "* Random guess? **Impossible too combinatorial**\n",
    "* Naive approach: **Orthogonal search**, very slow\n",
    "* Analytic solution: $$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}.$$\n",
    "Out of scope. It doesn't scale!\n",
    "* **Gradient descent**: Compute the gradient, do a little step toward the oposite direction of the gradient\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a0560",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The maths behind *gradient descent* are out of the scope of this class  \n",
    "\n",
    "**Intuition behind *gradient descent*: You are on top of a mountain, it's very foggy, you want to go back to the village; you take a step towards the steepest local descent**\n",
    "\n",
    "At the end of the day, you might end up somewhere. Where? Are you guaranteed to find the optimal solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339c567f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ -1.5617, -15.5320,   0.1981,  -1.5115,   5.4930,  -4.2655,  -0.2699,\n",
       "          17.9946,   4.9348,  -7.7862]),\n",
       " tensor([2.5000, 4.8000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_W = torch.Tensor([2.5, 4.8])\n",
    "input_data = torch.randn((10, 2))\n",
    "ground_truth = torch.matmul(input_data, true_W)\n",
    "ground_truth, true_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd10933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial guess tensor([ 0.0818, -1.3240], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.5010, 4.7989], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_weights = torch.randn((2), requires_grad=True)\n",
    "print(f'initial guess {random_weights}')\n",
    "epoch = 300\n",
    "for i in range(epoch):\n",
    "    prediction = torch.matmul(input_data, random_weights)\n",
    "    loss = criterion(prediction, ground_truth)\n",
    "    loss.backward()\n",
    "    # Why do we need to remove the gradient computation here ?\n",
    "    with torch.no_grad():\n",
    "        random_weights = random_weights - 0.01 * random_weights.grad\n",
    "    random_weights.requires_grad = True\n",
    "random_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81648669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytorch will write this loop for you automatically.\n",
    "\n",
    "But it's important to know how to ask for gradient computation and where the gradient is computed."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
