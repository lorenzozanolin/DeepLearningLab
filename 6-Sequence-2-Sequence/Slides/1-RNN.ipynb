{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbe562e",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640a5bd5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import collections\n",
    "import re\n",
    "\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593f5d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not all data are independant from each other  \n",
    "Thats's not the case for a lot of problems, such as stock prediction, weather forecast, natural language processing (NLP), video, audio conversation, etc.  \n",
    "\n",
    "CNN are powerful to process spatial information, Recurrent Neural Networks (RNN) were designed to better handle sequential information\n",
    "\n",
    "As text data are widely accessible, a lot of the focus on sequence to sequence model has been targeted towards text processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c6b07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='data/stock.png' width=\"65%\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='tradingview.com'>Tradingview</a></p>\n",
    "</center>\n",
    "\n",
    "To predict stock price at time step $t$ you need to consider the price from $x_1$ to $x_t-1$  \n",
    "When we are trying to predict the next stock price, we are predicting based on all historical data\n",
    "$$x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec240ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One problem with our current model is that the number of inputs $x_1,..x_t-1$ is variable from one input to another  \n",
    "Worse, the input can increase during training as for this stock prediction example!\n",
    "Of course we can't often compute all the $x_1,..x_t-1$ as it would lead to something too big to be computed (intractable)  \n",
    "So we often limit our computation to the last $n$ input example $x_t-n,..x_t-1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689fd9e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Models that predict series based on some inputs are called *auto-regressive models* as they perform regression on themselves  \n",
    "One common strategy is to keep a memory vector, to allow the model to store information there. Such models are called *latent auto-regressive models*\n",
    "<center>\n",
    "    <img src='data/sequence-model.svg' width=\"65%\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='d2l.ai'>D2L</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab230ea6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way to train this, is to pass historical observations to predict the next observation given the ones up to right now  \n",
    "This is pretty expensive as you have dependencies between each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a0f3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Depending on the task solved, different architecture are used\n",
    "\n",
    "<center>\n",
    "    <img src='http://karpathy.github.io/assets/rnn/diags.jpeg' width=\"65%\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'>Karpathy</a></p>\n",
    "</center>\n",
    "\n",
    "Each rectangle in the diagram represents a vector, and arrows represent functions (e.g. matrix multiplication).\n",
    "\n",
    "* Input vectors are shown in red.\n",
    "* Output vectors are shown in blue.\n",
    "* Green vectors represent the RNN's state (more on this soon).\n",
    "\n",
    "From left to right:\n",
    "\n",
    "1. Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification).\n",
    "2. Sequence output (e.g. image captioning takes an image and outputs a sentence of words).\n",
    "3. Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment).\n",
    "4. Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).\n",
    "5. Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).\n",
    "\n",
    "Notice that in every case there are no pre-specified constraints on the lengths of sequences because the recurrent transformation (green) is fixed and can be applied as many times as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac124ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Many to one example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2b0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ManyToOneRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(ManyToOneRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # (batch, sequence_length, X)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, h = self.rnn(x, h0)\n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f1229",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The many to many example follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b4f8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But to train, we need some training data  \n",
    "One big application of auto-regressive model is natural language processing  \n",
    "\n",
    "The text is preprocess by being tokenized, each words are assigned a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0298a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n",
      "number of lines in the text: 3221\n",
      "\n",
      "so most people think but wait a moment can an instantaneous\n"
     ]
    }
   ],
   "source": [
    "lines = read_time_machine()\n",
    "print(f'number of lines in the text: {len(lines)}\\n')\n",
    "print(lines[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6791229",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['you', 'must', 'follow', 'me', 'carefully', 'i', 'shall', 'have', 'to', 'controvert', 'one', 'or', 'two']\n",
      "['ideas', 'that', 'are', 'almost', 'universally', 'accepted', 'the', 'geometry', 'for']\n",
      "['instance', 'they', 'taught', 'you', 'at', 'school', 'is', 'founded', 'on', 'a', 'misconception']\n",
      "[]\n",
      "['is', 'not', 'that', 'rather', 'a', 'large', 'thing', 'to', 'expect', 'us', 'to', 'begin', 'upon']\n",
      "['said', 'filby', 'an', 'argumentative', 'person', 'with', 'red', 'hair']\n",
      "[]\n",
      "['i', 'do', 'not', 'mean', 'to', 'ask', 'you', 'to', 'accept', 'anything', 'without', 'reasonable']\n",
      "['ground', 'for', 'it', 'you', 'will', 'soon', 'admit', 'as', 'much', 'as', 'i', 'need', 'from', 'you', 'you']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines):\n",
    "    return [line.split() for line in lines]\n",
    "tokens = tokenize(lines)\n",
    "for i in range(20, 30):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187415d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, assigning a number to each of the word is not always pertinant  \n",
    "Some words appear very unfrequently, and so, to reduce the size of the dataset we often skip very unfrequent word\n",
    "\n",
    "Also, some special tokens are reserved for things such as `<pad>` for padding, `<bos>` for beginning and `<eos>` for an end of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51dbffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3221lines [00:00, 598920.65lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "v1 = torchtext.vocab.build_vocab_from_iterator(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc821aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 0, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1['you'], v1['<unk>'], v1['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57a7a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**: State-of-the-art models (GPT3 at the time I am writting these lines) do not tokenize this way. \n",
    "They employ a technique called [byte pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) which allows them to handle out-of-vocabulary words that were not present in the training dataset. \n",
    "Reference implementation: https://github.com/openai/tiktoken. \n",
    "Using BPE tokenization can significantly improve the performance of language models, as it allows them to handle a larger and more diverse vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335c9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The objective of a language model (i.e., a model used for natural language processing tasks) is to estimate the joint probability of a word sequence\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T).$$\n",
    "\n",
    "with $x_1, x_2, \\ldots, x_T$ a sequence of words of length $T$ forming a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b738a3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we obtain such model, we can easily generate text by prediction the conditional probability of the next work w.r.t to the beginning of the sentence\n",
    "\n",
    "$$x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1)$$\n",
    "\n",
    "\n",
    "The joint probability of a word sequence is defined by:\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^T P(x_t  \\mid  x_1, \\ldots, x_{t-1}).$$\n",
    "\n",
    "Example: $P(\\text{deep}, \\text{learning}, \\text{is}, \\text{fun}) =  P(\\text{deep}) P(\\text{learning}  \\mid  \\text{deep}) P(\\text{is}  \\mid  \\text{deep}, \\text{learning}) P(\\text{fun}  \\mid  \\text{deep}, \\text{learning}, \\text{is}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd000fb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a self-supervised training task.\n",
    "You can collect millions of text and train a model to predict next word each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f3dc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The problem with such approach is that the dependency $x_{t-1}, \\ldots, x_1$ can be very long  \n",
    "To allow models to store information, as we discuss previously, we often use *auto-regressive* models that store in their hidden state $h_{t-1}$ the sequence of information up to $t-1$ they need to predict at time-step $t$\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1})$$\n",
    "\n",
    "with $h_t$ defined recursively:\n",
    "$h_t = f(x_{t}, h_{t-1})$\n",
    "\n",
    "<center>\n",
    "    <img src='data/rnn.svg' width=\"65%\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='d2l.ai'>D2L</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8cbfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytorch contains pre-defined RNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708b6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(v1), num_hiddens, num_layers=2, nonlinearity='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7a438",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### By default, `nn.RNN` takes input of the form *(step, batch, input)*, if you want to pass the batch first, you have to specifiy it when building the RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1295d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(4581, 256, num_layers=2, batch_first=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.RNN(len(v1), num_hiddens, num_layers=2, nonlinearity='tanh', batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838a1df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recurrent layers are a bit special, in the sense that they don't just ouput the result of their operation but also their current state, which will be used in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1983927d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        # Trick to get the device of the model, see: https://stackoverflow.com/questions/58926054/how-to-get-the-device-type-of-a-pytorch-module-conveniently\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size=1):\n",
    "        # for iter 0 we need an initial state\n",
    "        return torch.zeros((self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                 device=self.dummy_param.device)\n",
    "    \n",
    "net = RNNModel(rnn_layer, vocab_size=len(v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492d16e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While predicting the next word is best for accuracy, it makes the model complex as the dimensionality of the output is huge (i.e., the number of words in the vocabulary)  \n",
    "To simplify things here, we will predict next character, making the problem easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provides iterator and vocab\n",
    "batch_size = 1\n",
    "num_steps = 18 #maximum sequence length\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31565d5c",
   "metadata": {},
   "source": [
    "`D2L` defines their own `Vocab` class and doesn't use `torchtext` official `Vocab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524bdca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We first need to initialize the state of the `RNN`  \n",
    "Usually we initialize the hidden state with a vector of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b4a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = net.begin_state(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2539a99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The typical use case for `RNN` is to pass a *prefix* sentence and ask it to generate the rest of the sentence  \n",
    "In that case, we usually *warm-start* the RNN by passing the prefix to get a *good* hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91caef00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tgrpppppppp'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'time traveller '\n",
    "outputs = [vocab[prefix[0]]]\n",
    "for y in prefix[1:]:  # Warm-up period\n",
    "    _, state = net(torch.LongTensor([outputs[-1]]).reshape(1, 1), state)\n",
    "    outputs.append(vocab[y])\n",
    "num_preds = 10\n",
    "outputs = [vocab[prefix[0]]]\n",
    "for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "    y, state = net(torch.LongTensor([outputs[-1]]).reshape(1, 1), state)\n",
    "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb38e9",
   "metadata": {},
   "source": [
    "As we could have imagine, without training, the text generated is purely random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ac339",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Trainning `RNN` is notoriously difficult, due to something called `Backpropagation Throught Time`  \n",
    "As we need to backpropagate throught previous iteration, the gradient flow goes $T$ times trought the networks layers  \n",
    "Leading to vanishing and exploding gradient problem, one common fix is to clip the norm of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c83c677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c247a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's briefly train this network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e797194d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m\n\u001B[1;32m      2\u001B[0m num_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m18\u001B[39m \u001B[38;5;66;03m#maximum sequence length\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m train_iter, vocab \u001B[38;5;241m=\u001B[39m \u001B[43md2l\u001B[49m\u001B[38;5;241m.\u001B[39mload_data_time_machine(batch_size, num_steps)\n\u001B[1;32m      4\u001B[0m rnn_layer \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mRNN(\u001B[38;5;28mlen\u001B[39m(vocab), num_hiddens)\n\u001B[1;32m      5\u001B[0m net \u001B[38;5;241m=\u001B[39m RNNModel(rnn_layer, vocab_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(vocab))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'd2l' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_steps = 18 #maximum sequence length\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens) # (seq, batch, X)\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "state = net.begin_state(batch_size=batch_size)\n",
    "num_epochs, num_steps = 1000, 18\n",
    "for epoch in range(num_epochs):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for X, Y in train_iter:\n",
    "        state = net.begin_state(batch_size=batch_size)\n",
    "        (output, state) = net(X, state)\n",
    "        y = torch.transpose(Y, 0, 1).reshape(-1)\n",
    "        l = criteria(output, y.long())\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        l_sum += l.item() * y.numel()\n",
    "        n += y.numel()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'epoch {epoch + 1}, loss {l_sum / n:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df29c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After training the output of the network should be a bit more coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d23d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tranled the med\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    state = net.begin_state(batch_size=1)\n",
    "    prefix = 'time traveller '\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(torch.LongTensor([outputs[-1]]).reshape(1, 1), state)\n",
    "        outputs.append(vocab[y])\n",
    "    num_preds = 18\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(torch.LongTensor([outputs[-1]]).reshape(1, 1), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    print(''.join([vocab.idx_to_token[i] for i in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef486b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "dbdcd3b24c586c20b309eff54aa9b697e80778f5a79c7346c0d2776399ed8b7c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
