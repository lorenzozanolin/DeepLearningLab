{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# From scratch implementation of the Transformer model in PyTorch.\n",
    "\n",
    "Further reading: [Attention is all you need](https://arxiv.org/abs/1706.03762) and [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "<img src=\"https://d2l.ai/_images/transformer.svg\" alt=\"The transformer architecture\" >"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings\n",
    "\n",
    "The embedding layer is the first layer of the Transformer model. The embedding layer converts the input tokens and target tokens into vectors of size $d_{\\text{model}}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Because Transformers models are order-invariant, they rely on the positional encoding to capture the position of the word in the sentence.\n",
    "The positional encoding is a function of the position of the word in the sentence. The positional encoding is added to the embedding of the input. The positional encoding is a vector of size $d_{\\text{model}}$ and is added to the embedding of the input.\n",
    "Positional encodings can either be learned or fixed. In this implementation, we use a fixed positional encoding.\n",
    "The fixed positional encoding add sine and cosine functions of different frequencies to the embedding of the input. The sine and cosine functions of different frequencies are used to capture the position of the word in the sentence.\n",
    "The transformer model will then learn to extract the position of the word from the positional encoding."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # register_buffer is a special kind of attribute that is not\n",
    "        # considered a model parameter, so that it will not be returned\n",
    "        # by model.parameters()\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "The multi-head attention layer is a key component of the Transformer model, composed of $h$ scaled dot-product attention sublayers. It takes query, key, and value vectors of size $d_{\\text{model}}$ as inputs, and projects them into $h$ smaller vectors before computing the scaled dot-product attention on each. The outputs of the $h$ attention sublayers are concatenated and transformed back to a vector of size $d_{\\text{model}}$. The multi-head attention layer is followed by residual connection and layer normalization. The recent implementation of pre-normalized multi-head attention places the layer normalization at the beginning of the sublayers (called \"Pre-Norm\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model // heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.heads, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.heads, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.heads, self.d_k)\n",
    "        # transpose to get dimensions bs * heads * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # calculate dot product attention\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = torch.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "The position-wise feed-forward network is a two-layer feed-forward network with a non-linear activation function in between (originally ReLU, recent application use Mish/GeLU). The position-wise feed-forward network is applied to each position separately and identically. The position-wise feed-forward network is followed by residual connection and layer normalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.gelu(self.w_1(x))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder and Decoder Stacks\n",
    "\n",
    "The encoder and decoder stacks are composed of $N$ identical layers. Each layer is composed of a multi-head attention layer, a position-wise feed-forward network, and residual connection and layer normalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def __call__(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = nn.Sequential(\n",
    "            ResidualConnection(size, dropout),\n",
    "            ResidualConnection(size, dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder Layer\n",
    "\n",
    "The decoder is composed of a stack of $N$ identical layers. Each layer is composed of a multi-head attention layer, a position-wise feed-forward network, and residual connection and layer normalization. The multi-head attention layer is applied to the output of the decoder stack, and the encoder output. The encoder output is used as the key and value vectors, while the output of the decoder stack is used as the query vector. The multi-head attention layer is followed by residual connection and layer normalization. The output of the decoder stack is then used as the query vector, and the output of the decoder stack is used as the key and value vectors. The multi-head attention layer is followed by residual connection and layer normalization. The output of the decoder stack is then used as the input to the position-wise feed-forward network. The position-wise feed-forward network is followed by residual connection and layer normalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = nn.Sequential(\n",
    "            ResidualConnection(size, dropout),\n",
    "            ResidualConnection(size, dropout),\n",
    "            ResidualConnection(size, dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder and Decoder Stacks\n",
    "\n",
    "The encoder and decoder stacks are composed of $N$ identical layers. Each layer is composed of a multi-head attention layer, a position-wise feed-forward network, and residual connection and layer normalization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generator\n",
    "\n",
    "The final output layer of the model is a linear layer and a softmax function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                           tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity check: The output of the encoder should be the same size as the input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    Encoder(EncoderLayer(512, MultiHeadAttention(8, 512), PositionwiseFeedForward(512, 2048), 0.1), 6),\n",
    "    Decoder(DecoderLayer(512, MultiHeadAttention(8, 512), MultiHeadAttention(8, 512), PositionwiseFeedForward(512, 2048), 0.1), 6),\n",
    "    nn.Sequential(Embeddings(512, 10000), PositionalEncoding(512, 0)),\n",
    "    nn.Sequential(Embeddings(512, 10000), PositionalEncoding(512, 0)),\n",
    "    Generator(512, 10000)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "long_input_data = torch.LongTensor(10, 32).random_(0, 10000)\n",
    "output = model.encode(long_input_data, None)\n",
    "assert output.size() == (10, 32, 512)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
