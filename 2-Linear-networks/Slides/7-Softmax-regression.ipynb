{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493be603",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# SoftMax regression\n",
    "\n",
    "From regression to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c72039",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3da12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than predicting quantities, we often want to classify things.\n",
    "\n",
    "**Example**: Classify a mail as a spam or not, is there a cat in this image ?, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d497ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**In case of a binary classification**, we only provide one output unit with a Sigmoid applied to it\n",
    "\n",
    "**Why don't we have 2 output units?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381bd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4159f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"images/sigmoid.png\" height=\"70%\" width=\"70%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80769930",
   "metadata": {},
   "source": [
    "When you want to predict, you apply a threshold (usually 0.5) to know which category was predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bba256",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In case of a binary classification with one output unit, we use the **Binary Cross Entropy** loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b26bb12",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ef5ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we have more than one classes, we represent them using encoding\n",
    "\n",
    "This encoding ensure there are no order in the representation\n",
    "if for **{dog, cat, bird, fish}** we were assigning $y \\in \\{1, 2, 3, 4\\}$ we would have assign an **order** and a **value** to each class. We don't want that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ce57c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The usual way to represent categorical data is the *one-hot encoding*.\n",
    "\n",
    "It is a vector with as many components as we have categories.\n",
    "\n",
    "The component corresponding to particular instance's category is set to 1\n",
    "and all other components are set to 0.\n",
    "\n",
    "$$y \\in \\{(1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1)\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07b0d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To estimate the conditional probabilities of all the possible classes, we need a model with one output per class\n",
    "\n",
    "<center><img src=\"images/softmaxreg.svg\" height=\"70%\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129a38c",
   "metadata": {},
   "source": [
    "Suppose that the entire dataset $\\{\\mathbf{X}, \\mathbf{Y}\\}$ has $n$ examples,\n",
    "where the example indexed by $i$\n",
    "consists of a feature vector $\\mathbf{x}^{(i)}$ and a one-hot label vector $\\mathbf{y}^{(i)}$.\n",
    "We can compare the estimates with reality\n",
    "by checking how probable the actual classes are\n",
    "according to our model, given the features:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "**If $P(\\mathbf{Y} \\mid \\mathbf{X}) = 1$ we have a perfect model!** \n",
    "\n",
    "We want to *maximize* the maximum likelihood. However, in neural network, we want to have a loss we can *minimize*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378f96f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Minimizing the **negative log-likelihood** is equivalent to maximizing the maximum likelihood\n",
    "\n",
    "This loss is called the **cross-entropy loss**.\n",
    "It takes the output layer (called **logit**) and the ground truth, transform them into probabilities and compares with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e9fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 1])\n",
      "tensor([[ 0.9347, -1.3496,  0.6083,  0.1674,  0.9082],\n",
      "        [-0.7064, -1.2840, -1.0878, -0.3773,  0.9027],\n",
      "        [ 1.6767,  0.5550,  0.2900, -2.2211, -0.1674]])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "prediction = torch.randn(3, 5) # the per category-logits you've predicted \n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = criterion(prediction, target)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bbe86",
   "metadata": {},
   "source": [
    "**Tip**: The CrossEntropyLoss allows you to assign weight to each class, it can be usefull if your dataset is **unbalanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf261dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**For multi class classification problem**: Our model output scalars, we want probabilities.\n",
    "These scalars are called **logits**.\n",
    "\n",
    "To transform a vector of **logits** into a probability vector, we use the **SoftMax** function\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e9090",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need a loss function capable to mesure the quality of our predicted probabilities\n",
    "\n",
    "We rely on the **maximum likelihood** estimation\n",
    "\n",
    "**Softmax** provides a vector $\\hat{\\mathbf{y}}$,\n",
    "which we can interpret as estimated conditional probabilities\n",
    "of each class given any input $\\mathbf{x}$, e.g.,\n",
    "$\\hat{y}_1$ = $P(y=\\text{cat} \\mid \\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265eb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1) # what is dim 0, what is dim 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf49a83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ⚠️ Cross-Entropy itself apply a SoftMax, if your model outputs probabilities, use the Negative Log Likelood loss ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762a393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f49c2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you want to predict, you simply take the index with the maximum probability as the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2253f8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.2424, 0.5465, 0.5827, 0.4447]),\n",
       "indices=tensor([3, 3, 3, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "torch.max(a, dim=1) # torch.max return the maximum value and the corresponding index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba279a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Minimizing the **negative log-likelihood** is equivalent to maximizing the maximum likelihood\n",
    "\n",
    "$$\n",
    "-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n",
    "$$\n",
    "\n",
    "where for any pair of label $\\mathbf{y}$ and model prediction $\\hat{\\mathbf{y}}$ over $q$ classes,\n",
    "the loss function $l$ is\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n",
    "\n",
    "This loss is called the **cross-entropy loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d67830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6341,  0.3860,  0.5390],\n",
       "        [-0.5015,  0.6337,  0.6364],\n",
       "        [-0.3050,  0.1935,  0.8646],\n",
       "        [-0.6423,  0.7129, -0.4183],\n",
       "        [-0.2103, -0.6932,  1.1270],\n",
       "        [-0.3464,  0.6114,  0.1044],\n",
       "        [-0.2144, -0.3100,  0.3348],\n",
       "        [-0.0124, -0.6519, -0.2480],\n",
       "        [-0.9427,  2.0335,  1.3515],\n",
       "        [-0.5265,  0.5125,  0.9804]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(4, 3))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "input_features = torch.randn(10, 4)\n",
    "targets = torch.empty(10, dtype=torch.long).random_(3)\n",
    "model(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19079fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss now 1.3080755472183228\n",
      "loss now 1.3068199157714844\n",
      "loss now 1.3055660724639893\n",
      "loss now 1.3043138980865479\n",
      "loss now 1.303063154220581\n",
      "loss now 1.3018144369125366\n",
      "loss now 1.300567388534546\n",
      "loss now 1.2993220090866089\n",
      "loss now 1.2980785369873047\n",
      "loss now 1.2968366146087646\n",
      "loss now 1.2955964803695679\n",
      "loss now 1.294358253479004\n",
      "loss now 1.293121576309204\n",
      "loss now 1.291886806488037\n",
      "loss now 1.2906538248062134\n",
      "loss now 1.2894222736358643\n",
      "loss now 1.2881925106048584\n",
      "loss now 1.2869646549224854\n",
      "loss now 1.285738468170166\n",
      "loss now 1.2845139503479004\n",
      "loss now 1.2832914590835571\n",
      "loss now 1.2820703983306885\n",
      "loss now 1.280851125717163\n",
      "loss now 1.279633641242981\n",
      "loss now 1.2784178256988525\n",
      "loss now 1.277203917503357\n",
      "loss now 1.2759915590286255\n",
      "loss now 1.2747809886932373\n",
      "loss now 1.273572325706482\n",
      "loss now 1.2723652124404907\n",
      "loss now 1.2711598873138428\n",
      "loss now 1.269956350326538\n",
      "loss now 1.2687546014785767\n",
      "loss now 1.2675542831420898\n",
      "loss now 1.266356110572815\n",
      "loss now 1.2651593685150146\n",
      "loss now 1.2639644145965576\n",
      "loss now 1.2627713680267334\n",
      "loss now 1.261579990386963\n",
      "loss now 1.260390281677246\n",
      "loss now 1.259202241897583\n",
      "loss now 1.2580161094665527\n",
      "loss now 1.2568315267562866\n",
      "loss now 1.2556489706039429\n",
      "loss now 1.2544679641723633\n",
      "loss now 1.2532886266708374\n",
      "loss now 1.2521111965179443\n",
      "loss now 1.2509353160858154\n",
      "loss now 1.2497613430023193\n",
      "loss now 1.248589038848877\n",
      "loss now 1.2474185228347778\n",
      "loss now 1.2462495565414429\n",
      "loss now 1.2450824975967407\n",
      "loss now 1.2439171075820923\n",
      "loss now 1.242753505706787\n",
      "loss now 1.2415916919708252\n",
      "loss now 1.240431308746338\n",
      "loss now 1.2392730712890625\n",
      "loss now 1.2381162643432617\n",
      "loss now 1.2369612455368042\n",
      "loss now 1.2358081340789795\n",
      "loss now 1.234656572341919\n",
      "loss now 1.2335067987442017\n",
      "loss now 1.232358694076538\n",
      "loss now 1.2312123775482178\n",
      "loss now 1.2300677299499512\n",
      "loss now 1.2289249897003174\n",
      "loss now 1.2277839183807373\n",
      "loss now 1.2266443967819214\n",
      "loss now 1.2255069017410278\n",
      "loss now 1.2243707180023193\n",
      "loss now 1.2232365608215332\n",
      "loss now 1.2221041917800903\n",
      "loss now 1.220973253250122\n",
      "loss now 1.2198443412780762\n",
      "loss now 1.2187169790267944\n",
      "loss now 1.2175915241241455\n",
      "loss now 1.2164674997329712\n",
      "loss now 1.2153456211090088\n",
      "loss now 1.2142250537872314\n",
      "loss now 1.2131065130233765\n",
      "loss now 1.2119895219802856\n",
      "loss now 1.210874319076538\n",
      "loss now 1.2097607851028442\n",
      "loss now 1.208648920059204\n",
      "loss now 1.2075388431549072\n",
      "loss now 1.2064305543899536\n",
      "loss now 1.2053238153457642\n",
      "loss now 1.204219102859497\n",
      "loss now 1.203115701675415\n",
      "loss now 1.2020143270492554\n",
      "loss now 1.2009146213531494\n",
      "loss now 1.1998165845870972\n",
      "loss now 1.1987202167510986\n",
      "loss now 1.197625756263733\n",
      "loss now 1.1965328454971313\n",
      "loss now 1.1954416036605835\n",
      "loss now 1.194352149963379\n",
      "loss now 1.1932644844055176\n",
      "loss now 1.1921783685684204\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=3e-3)\n",
    "epochs = 100\n",
    "for idx_epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    Y_pred = model(input_features)\n",
    "    loss = criterion(Y_pred, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'loss now {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e19ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
