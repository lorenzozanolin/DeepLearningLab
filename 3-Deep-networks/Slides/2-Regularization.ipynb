{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dcb3df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "Great power implies great responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59eb2111",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ab170",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Powerfull models have a risk to overfit\n",
    "\n",
    "**Think about it**: We are trying to reduce the error on the training set to get a good result on the validation/test sets\n",
    "\n",
    "Best way to reduce the error: **Memorize everything**. This is not what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9abe08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='images/capacity-vs-error.svg' width='60%'/>\n",
    "    <p>Source: <a href='d2l.ai'>d2l.ai</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d1ff58",
   "metadata": {},
   "source": [
    "The more the model is complex, the easier it is for it to memorize (i.e., **overfit**) the training set.\n",
    "\n",
    "However, if the model is too simple, it will failt to learn and will not have good performance (i.e., **underfit**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aead478",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to overcome this behavior?\n",
    "\n",
    "<ol>\n",
    "    <li><b>Increase the number of elements in the training set:</b> The more things you have to memorize the harder. <br />In practice, good quality datas are <b>very hard and expensive</b> to get.\n",
    "    </li>\n",
    "    <li><b>Reduce the complexity of your model:</b> You don't need big and deep neural network for simple problems.</li>\n",
    "    <li><b>Regularization techniques</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4218db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization techniques\n",
    "\n",
    "One of the most *classic* approaches is to use **weight decay** techniques.\n",
    "The idea is pretty straightforward. We add a second objective to the loss function to penalize too complex models.\n",
    "\n",
    "The problem objective became: reduce the error of the neural network + reduce the complexity of the neural network\n",
    "\n",
    "The network will try to find an *equilibrium* between these two objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f356bfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most popular weight decay technique is the **L2 regularization term**\n",
    "We simply sum the square of the neural network weights $$\\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "In Pytorch, this is defined in the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482e92b7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd94def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f7729",
   "metadata": {},
   "source": [
    "In Pytorch the **weight decay** is an additional gradient on top of the loss gradient -> **Don't put 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8695e",
   "metadata": {},
   "source": [
    "If you use adaptive gradient algorithm such as the **Adam** optimizer with weight decay, you need to use the **AdamW** optimizer to avoid coupling problem between the L2 regularization and the adaptive learning rate.  \n",
    "More info here: https://arxiv.org/abs/1711.05101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6152983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df0c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**L1 norm** (i.e. sum of absolute term) exists but is less popular and need to be computed by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c80f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "l1_alpha = 1e-3\n",
    "l1_term = torch.tensor(0.)\n",
    "for param in model.parameters():\n",
    "    l1_term += torch.norm(param, 1)\n",
    "loss += l1_alpha * l1_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951162a5",
   "metadata": {},
   "source": [
    "What is the impact of **L2 norm vs L1 norm**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05875c62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "When a neural network overfit, it memorizes inside its weights the training set (acts as a database)\n",
    "\n",
    "**Idea**: Inject noise in the layers to deactivated randomly a certain proportion of the neurons per layers. The neural network can't memorize because the neuron might be turned off at a given iteration\n",
    "\n",
    "<center>\n",
    "    <img src='images/dropout2.svg' width='60%'/>\n",
    "    <p>Source: <a href='http://d2l.ai'>d2l.ai</a></p>\n",
    "</center>\n",
    "\n",
    "Dropout should be place after activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4120b749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with_dropout = nn.Sequential(\n",
    "    nn.Linear(18, 36),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(36, 36),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(36, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea5d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Dropout is only used during training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54138f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=18, out_features=36, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=36, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the network in training mode\n",
    "with_dropout.train()\n",
    "\n",
    "# after training, but it to evaluation mode\n",
    "with_dropout.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b3681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
