{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# IMDB sentiment analysis with RNNs\n",
    "\n",
    "Kaggle: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "# allows to have a progress bar in pandas, useful for long processing operations\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Read the dataset and observe the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lucky us, the dataset is well-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Transform the labels to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_label(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "\n",
    "data['label'] = data['sentiment'].progress_apply(transform_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "- In classic NLP, the text is often preprocessed to remove tokens that might confuse the classifier\n",
    "- Below you can find some examples of possible preprocessing techniques\n",
    "- Feel free to modify them to improve the results of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def rm_link(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "\n",
    "# handle case like \"shut up okay?Im only 10 years old\"\n",
    "# become \"shut up okay Im only 10 years old\"\n",
    "def rm_punct2(text):\n",
    "    # return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "\n",
    "\n",
    "def rm_html(text):\n",
    "    # remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove <br /> tags\n",
    "    return re.sub(r'<br />', '', text)\n",
    "\n",
    "\n",
    "def space_bt_punct(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)  # add whitespaces between punctuation\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)  # remove double whitespaces\n",
    "    return s\n",
    "\n",
    "\n",
    "def rm_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def rm_whitespaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "\n",
    "def rm_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "\n",
    "\n",
    "def spell_correction(text):\n",
    "    # if too slow: return text\n",
    "    return text\n",
    "    # https://pypi.org/project/pyspellchecker/\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            candidate = spell.correction(word)\n",
    "            if candidate is not None:\n",
    "                corrected_text.append(candidate)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "def clean_pipeline(text):\n",
    "    text = text.lower()\n",
    "    no_link = rm_link(text)\n",
    "    no_html = rm_html(no_link)\n",
    "    space_punct = space_bt_punct(no_html)\n",
    "    no_punct = rm_punct2(space_punct)\n",
    "    no_number = rm_number(no_punct)\n",
    "    no_whitespaces = rm_whitespaces(no_number)\n",
    "    no_nonasci = rm_nonascii(no_whitespaces)\n",
    "    no_emoji = rm_emoji(no_nonasci)\n",
    "    spell_corrected = spell_correction(no_emoji)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's clean the reviews first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['review'] = data['review'].progress_apply(clean_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We now tokenize and remove stopwords (i.e. the, a, an, etc.) and lemmatize the words (i.e. running -> run, better -> good, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def rm_stopwords(text):\n",
    "    return [i for i in text if i not in stopwords]\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "    # make sure lemmas does not contains stopwords\n",
    "    return rm_stopwords(lemmas)\n",
    "\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "    tokens = tokenize(text)\n",
    "    no_stopwords = rm_stopwords(tokens)\n",
    "    lemmas = lemmatize(no_stopwords)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['review'] = data['review'].progress_apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embedding\n",
    "\n",
    "- ANNs cannot process text input\n",
    "- Input tokens must be mapped to integers using a vocabulary\n",
    "- In this example, we build a vocabulary manually, but you can also replace this code with an [embedding layer](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all processed reviews\n",
    "reviews = data.review.values\n",
    "# merge into single variable, separated by whitespaces\n",
    "words = ' '.join(reviews)\n",
    "# obtain list of words\n",
    "words = words.split()\n",
    "# build vocabulary\n",
    "counter = Counter(words)\n",
    "# only keep top 2000 words\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)[:2000]\n",
    "int2word = dict(enumerate(vocab, 2))\n",
    "int2word[0] = '<PAD>'\n",
    "int2word[1] = '<UNK>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_enc = [[word2int[word] if word in word2int else word2int['<UNK>'] for word in review.split()] for review in tqdm(reviews, desc='encoding')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because we have to build batch, we have to pad the reviews to the same length. We will pad the reviews with <PAD> token.\n",
    "**Because we use RNNs, we need to left pad and not right pad the sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# left padding sequences\n",
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    # features = np.zeros((len(reviews), seq_length), dtype=int)\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        start_index = max(0, seq_length - len(row))\n",
    "        # if seq_length < len(row) then review will be trimmed\n",
    "        features[i, start_index:] = np.array(row)[:min(seq_length, len(row))]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "seq_length = 128\n",
    "features = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = data.label.to_numpy()\n",
    "\n",
    "# train test split\n",
    "train_size = .75  # we will use 75% of whole data as train set\n",
    "val_size = .5  # and we will use 50% of test set as validation set\n",
    "\n",
    "# stratify will make sure that train and test set have same distribution of labels\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=1 - train_size, stratify=labels)\n",
    "\n",
    "# split test set into validation and test set\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=val_size, stratify=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# create tensor datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
