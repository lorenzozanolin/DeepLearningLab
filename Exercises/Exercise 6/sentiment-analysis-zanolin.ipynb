{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMDB sentiment analysis with RNNs\n\nKaggle: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\n#from spellchecker import SpellChecker\nfrom tqdm import tqdm\n# allows to have a progress bar in pandas, useful for long processing operations\ntqdm.pandas()\nfrom collections import Counter\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"a\")\n\nimport wandb\nwandb.login(key=secret_value_0)\nwandb.init(project='sentiment_analysis', save_code=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:05:32.550274Z","iopub.execute_input":"2024-01-29T16:05:32.550547Z","iopub.status.idle":"2024-01-29T16:06:17.189374Z","shell.execute_reply.started":"2024-01-29T16:05:32.550523Z","shell.execute_reply":"2024-01-29T16:06:17.188506Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlorenzozanolin-52\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240129_160545-cb0c9n9y</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lorenzozanolin-52/sentiment_analysis/runs/cb0c9n9y' target=\"_blank\">classic-water-8</a></strong> to <a href='https://wandb.ai/lorenzozanolin-52/sentiment_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lorenzozanolin-52/sentiment_analysis' target=\"_blank\">https://wandb.ai/lorenzozanolin-52/sentiment_analysis</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lorenzozanolin-52/sentiment_analysis/runs/cb0c9n9y' target=\"_blank\">https://wandb.ai/lorenzozanolin-52/sentiment_analysis/runs/cb0c9n9y</a>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lorenzozanolin-52/sentiment_analysis/runs/cb0c9n9y?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79c97bc7b5e0>"},"metadata":{}}]},{"cell_type":"markdown","source":"Read the dataset and observe the first 5 rows.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndata.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:06:17.191069Z","iopub.execute_input":"2024-01-29T16:06:17.191409Z","iopub.status.idle":"2024-01-29T16:06:19.251126Z","shell.execute_reply.started":"2024-01-29T16:06:17.191383Z","shell.execute_reply":"2024-01-29T16:06:19.250212Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Lucky us, the dataset is well-balanced.","metadata":{}},{"cell_type":"code","source":"data.sentiment.value_counts()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:06:19.252271Z","iopub.execute_input":"2024-01-29T16:06:19.252616Z","iopub.status.idle":"2024-01-29T16:06:19.638064Z","shell.execute_reply.started":"2024-01-29T16:06:19.252585Z","shell.execute_reply":"2024-01-29T16:06:19.637186Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"sentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Transform the labels to 0 and 1.","metadata":{}},{"cell_type":"code","source":"def transform_label(label):\n    return 1 if label == 'positive' else 0\n\n\ndata['label'] = data['sentiment'].progress_apply(transform_label)\ndata.head","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:06:19.640729Z","iopub.execute_input":"2024-01-29T16:06:19.641692Z","iopub.status.idle":"2024-01-29T16:06:20.207074Z","shell.execute_reply.started":"2024-01-29T16:06:19.641657Z","shell.execute_reply":"2024-01-29T16:06:20.206199Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"100%|██████████| 50000/50000 [00:00<00:00, 471902.14it/s]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of                                                   review sentiment  label\n0      One of the other reviewers has mentioned that ...  positive      1\n1      A wonderful little production. <br /><br />The...  positive      1\n2      I thought this was a wonderful way to spend ti...  positive      1\n3      Basically there's a family where a little boy ...  negative      0\n4      Petter Mattei's \"Love in the Time of Money\" is...  positive      1\n...                                                  ...       ...    ...\n49995  I thought this movie did a down right good job...  positive      1\n49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative      0\n49997  I am a Catholic taught in parochial elementary...  negative      0\n49998  I'm going to have to disagree with the previou...  negative      0\n49999  No one expects the Star Trek movies to be high...  negative      0\n\n[50000 rows x 3 columns]>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing\n\n- In classic NLP, the text is often preprocessed to remove tokens that might confuse the classifier\n- Below you can find some examples of possible preprocessing techniques\n- Feel free to modify them to improve the results of your classifier","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('omw-1.4')\nstopwords = set(stopwords.words('english'))\n\ndef rm_link(text):\n    return re.sub(r'http\\S+', '', text)\n\n\n# handle case like \"shut up okay?Im only 10 years old\"\n# become \"shut up okay Im only 10 years old\"\ndef rm_punct2(text):\n    # return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n\n\ndef rm_html(text):\n    # remove html tags\n    text = re.sub(r'<.*?>', '', text)\n    # remove <br /> tags\n    return re.sub(r'<br />', '', text)\n\n\ndef space_bt_punct(text):\n    pattern = r'([.,!?-])'\n    s = re.sub(pattern, r' \\1 ', text)  # add whitespaces between punctuation\n    s = re.sub(r'\\s{2,}', ' ', s)  # remove double whitespaces\n    return s\n\n\ndef rm_number(text):\n    return re.sub(r'\\d+', '', text)\n\n\ndef rm_whitespaces(text):\n    return re.sub(r'\\s+', ' ', text)\n\n\ndef rm_nonascii(text):\n    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n\n\ndef rm_emoji(text):\n    emojis = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE\n    )\n    return emojis.sub(r'', text)\n\ndef rm_contractions(text): #removes contracted form and insert normal form, i.e. he's -> he is\n    text=re.sub(\"isn't\",'is not',text)\n    text=re.sub(\"he's\",'he is',text)\n    text=re.sub(\"wasn't\",'was not',text)\n    text=re.sub(\"there's\",'there is',text)\n    text=re.sub(\"couldn't\",'could not',text)\n    text=re.sub(\"won't\",'will not',text)\n    text=re.sub(\"they're\",'they are',text)\n    text=re.sub(\"she's\",'she is',text)\n    text=re.sub(\"There's\",'there is',text)\n    text=re.sub(\"wouldn't\",'would not',text)\n    text=re.sub(\"haven't\",'have not',text)\n    text=re.sub(\"That's\",'That is',text)\n    text=re.sub(\"you've\",'you have',text)\n    text=re.sub(\"He's\",'He is',text)\n    text=re.sub(\"what's\",'what is',text)\n    text=re.sub(\"weren't\",'were not',text)\n    text=re.sub(\"we're\",'we are',text)\n    text=re.sub(\"hasn't\",'has not',text)\n    text=re.sub(\"you'd\",'you would',text)\n    text=re.sub(\"shouldn't\",'should not',text)\n    text=re.sub(\"let's\",'let us',text)\n    text=re.sub(\"they've\",'they have',text)\n    text=re.sub(\"You'll\",'You will',text)\n    text=re.sub(\"i'm\",'i am',text)\n    text=re.sub(\"we've\",'we have',text)\n    text=re.sub(\"it's\",'it is',text)\n    text=re.sub(\"don't\",'do not',text)\n    text=re.sub(\"that´s\",'that is',text)\n    text=re.sub(\"I´m\",'I am',text)\n    text=re.sub(\"it’s\",'it is',text)\n    text=re.sub(\"she´s\",'she is',text)\n    text=re.sub(\"he’s'\",'he is',text)\n    text=re.sub('I’m','I am',text)\n    text=re.sub('I’d','I did',text)\n    text=re.sub(\"he’s'\",'he is',text)\n    text=re.sub('there’s','there is',text)\n    return text\n\ndef spell_correction(text):\n    # if too slow: return text\n    return text\n    # https://pypi.org/project/pyspellchecker/\n    spell = 0#SpellChecker()\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            candidate = spell.correction(word)\n            if candidate is not None:\n                corrected_text.append(candidate)\n            else:\n                corrected_text.append(word)\n        else:\n            corrected_text.append(word)\n    return ' '.join(corrected_text)\n\ndef clean_pipetext(text):\n    text = text.lower()\n    no_link = rm_link(text)\n    no_cont = rm_contractions(no_link)\n    no_html = rm_html(no_cont)\n    space_punct = space_bt_punct(no_html)\n    no_punct = rm_punct2(space_punct)\n    no_number = rm_number(no_punct)\n    no_whitespaces = rm_whitespaces(no_number)\n    no_nonasci = rm_nonascii(no_whitespaces)\n    no_emoji = rm_emoji(no_nonasci)\n    spell_corrected = spell_correction(no_emoji)\n    return spell_corrected","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:06:20.210896Z","iopub.execute_input":"2024-01-29T16:06:20.211264Z","iopub.status.idle":"2024-01-29T16:06:21.169390Z","shell.execute_reply.started":"2024-01-29T16:06:20.211231Z","shell.execute_reply":"2024-01-29T16:06:21.168459Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's clean the reviews first:","metadata":{}},{"cell_type":"code","source":"data['review'] = data['review'].progress_apply(clean_pipetext)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:06:21.170764Z","iopub.execute_input":"2024-01-29T16:06:21.171352Z","iopub.status.idle":"2024-01-29T16:06:47.213247Z","shell.execute_reply.started":"2024-01-29T16:06:21.171288Z","shell.execute_reply":"2024-01-29T16:06:47.212456Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 50000/50000 [00:25<00:00, 1947.52it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We now tokenize and remove stopwords (i.e. the, a, an, etc.) and lemmatize the words (i.e. running -> run, better -> good, etc.).","metadata":{}},{"cell_type":"code","source":"!python3 -m nltk.downloader wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\n# preprocessing\ndef tokenize(text):\n    return word_tokenize(text)\n\n\ndef rm_stopwords(text):\n    return [i for i in text if i not in stopwords]\n\n\ndef lemmatize(text):\n    lemmatizer = WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(t) for t in text]\n    # make sure lemmas does not contains stopwords\n    return rm_stopwords(lemmas)\n\n\ndef preprocess_pipetext(text):\n    tokens = tokenize(text)\n    no_stopwords = rm_stopwords(tokens)\n    lemmas = lemmatize(no_stopwords)\n    return ' '.join(lemmas)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:06:47.214485Z","iopub.execute_input":"2024-01-29T16:06:47.214895Z","iopub.status.idle":"2024-01-29T16:06:50.967273Z","shell.execute_reply.started":"2024-01-29T16:06:47.214868Z","shell.execute_reply":"2024-01-29T16:06:50.966371Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"data['review'] = data['review'].progress_apply(preprocess_pipetext)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:06:50.968860Z","iopub.execute_input":"2024-01-29T16:06:50.969222Z","iopub.status.idle":"2024-01-29T16:10:08.023889Z","shell.execute_reply.started":"2024-01-29T16:06:50.969184Z","shell.execute_reply":"2024-01-29T16:10:08.023042Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 50000/50000 [03:16<00:00, 254.28it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's check the result.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:10:08.025027Z","iopub.execute_input":"2024-01-29T16:10:08.025293Z","iopub.status.idle":"2024-01-29T16:10:08.417062Z","shell.execute_reply.started":"2024-01-29T16:10:08.025269Z","shell.execute_reply":"2024-01-29T16:10:08.416245Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment  label\n0  one reviewer mentioned watching oz episode hoo...  positive      1\n1  wonderful little production . filming techniqu...  positive      1\n2  thought wonderful way spend time hot summer we...  positive      1\n3  basically family little boy jake think zombie ...  negative      0\n4  petter mattei love time money visually stunnin...  positive      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one reviewer mentioned watching oz episode hoo...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wonderful little production . filming techniqu...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thought wonderful way spend time hot summer we...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically family little boy jake think zombie ...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei love time money visually stunnin...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Embedding\n\n- ANNs cannot process text input\n- Input tokens must be mapped to integers using a vocabulary\n- In this example, we build a vocabulary manually, but you can also replace this code with an [embedding layer](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)","metadata":{}},{"cell_type":"code","source":"# get all processed reviews\nreviews = data.review.values\n# merge into single variable, separated by whitespaces\nwords = ' '.join(reviews)\n# obtain list of words\nwords = words.split()\n# build vocabulary\ncounter = Counter(words)\n# only keep top 2000 words\nvocab = sorted(counter, key=counter.get, reverse=True)[:2000]\nint2word = dict(enumerate(vocab, 2))\nint2word[0] = '<PAD>'\nint2word[1] = '<UNK>'\nword2int = {word: id for id, word in int2word.items()}","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:10:08.421167Z","iopub.execute_input":"2024-01-29T16:10:08.421450Z","iopub.status.idle":"2024-01-29T16:10:10.554874Z","shell.execute_reply.started":"2024-01-29T16:10:08.421426Z","shell.execute_reply":"2024-01-29T16:10:10.553990Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"reviews_enc = [[word2int[word] if word in word2int else word2int['<UNK>'] for word in review.split()] for review in tqdm(reviews, desc='encoding')]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:10:10.557197Z","iopub.execute_input":"2024-01-29T16:10:10.557574Z","iopub.status.idle":"2024-01-29T16:10:13.932921Z","shell.execute_reply.started":"2024-01-29T16:10:10.557538Z","shell.execute_reply":"2024-01-29T16:10:13.932062Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"encoding: 100%|██████████| 50000/50000 [00:02<00:00, 21143.91it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Because we have to build batch, we have to pad the reviews to the same length. We will pad the reviews with <PAD> token.\n**Because we use RNNs, we need to left pad and not right pad the sequence.**","metadata":{}},{"cell_type":"code","source":"# left padding sequences\ndef pad_features(reviews, pad_id, seq_length=128):\n    # features = np.zeros((len(reviews), seq_length), dtype=int)\n    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n\n    for i, row in enumerate(reviews):\n        start_index = max(0, seq_length - len(row))\n        # if seq_length < len(row) then review will be trimmed\n        features[i, start_index:] = np.array(row)[:min(seq_length, len(row))]\n\n    return features\n\n\nseq_length = 128\nfeatures = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:10:13.934306Z","iopub.execute_input":"2024-01-29T16:10:13.934705Z","iopub.status.idle":"2024-01-29T16:10:15.230891Z","shell.execute_reply.started":"2024-01-29T16:10:13.934680Z","shell.execute_reply":"2024-01-29T16:10:15.229766Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Split the data","metadata":{}},{"cell_type":"code","source":"labels = data.label.to_numpy()\n\n# train test split\ntrain_size = .75  # we will use 75% of whole data as train set\nval_size = .5  # and we will use 50% of test set as validation set\n\n# stratify will make sure that train and test set have same distribution of labels\ntrain_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=1 - train_size, stratify=labels)\n\n# split test set into validation and test set\nval_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=val_size, stratify=test_y)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:10:15.232050Z","iopub.execute_input":"2024-01-29T16:10:15.232361Z","iopub.status.idle":"2024-01-29T16:10:15.794081Z","shell.execute_reply.started":"2024-01-29T16:10:15.232331Z","shell.execute_reply":"2024-01-29T16:10:15.793227Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Define the datasets and dataloaders.","metadata":{}},{"cell_type":"code","source":"# define batch size\nbatch_size = 128\n\nwandb.log({'batch_size': batch_size})\n\n# create tensor datasets\ntrain_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_dataset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_dataset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# create dataloaders\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:10:15.795196Z","iopub.execute_input":"2024-01-29T16:10:15.795520Z","iopub.status.idle":"2024-01-29T16:10:16.253098Z","shell.execute_reply.started":"2024-01-29T16:10:15.795490Z","shell.execute_reply":"2024-01-29T16:10:16.252254Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Define the model.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass ManyToOneRNN(nn.Module):\n    def __init__(self, input_size, emb_size, hidden_size, num_layers, num_classes):\n        super(ManyToOneRNN, self).__init__()\n        # Embedding layer to convert input indices to dense vectors\n        self.embedding = nn.Embedding(input_size, emb_size) #in this case input size is the size of the vocabulary\n        # set the hidden size and the number of layers for the RNN\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        # we will use a RNN with a final Fully connected layer to predict the output (class positive or negative)\n        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True) # (batch, sequence_length, X)\n        self.fc = nn.Linear(hidden_size, num_classes)\n        # weights initialization for the fully connected layer\n        #nn.init.xavier_normal_(self.fc.weight)\n    \n    def forward(self, x):\n        # Embedding layer to convert input indices to dense vectors\n        x = self.embedding(x)\n        # RNN layer will output the hidden state and the output\n        rnn_out, h_n = self.rnn(x)\n        # Assuming h_n is a tuple of hidden states from all layers\n        # Concatenate the hidden states from all layers (assuming the last layer [-1])\n        h_n = h_n[-1].squeeze(0)\n        # Pass the concatenated hidden states through the fully connected layer\n        out = self.fc(h_n)\n        \n        return out","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:10:16.254313Z","iopub.execute_input":"2024-01-29T16:10:16.254639Z","iopub.status.idle":"2024-01-29T16:10:16.720912Z","shell.execute_reply.started":"2024-01-29T16:10:16.254605Z","shell.execute_reply":"2024-01-29T16:10:16.720056Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Instantiate the model.","metadata":{}},{"cell_type":"code","source":"wandb.init()\nemb_size = 256  # Dimension of the word embeddings\nhidden_size = 128  # Number of features in the hidden state\noutput_size = 1  # Dimension of the output, e.g., for a binary classification problem\n\nwandb.log({'Embedding Size': emb_size})\n\nmodel = ManyToOneRNN(input_size=len(word2int),emb_size=emb_size,hidden_size=hidden_size,num_layers=1,num_classes=output_size)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'  #we want to move the net on the GPU\nmodel = model.to(device)\nif device == 'cuda':\n    model = torch.nn.DataParallel(model) # if multiple GPUs use them\n    \nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T16:22:19.754918Z","iopub.execute_input":"2024-01-29T16:22:19.755305Z","iopub.status.idle":"2024-01-29T16:22:50.516950Z","shell.execute_reply.started":"2024-01-29T16:22:19.755263Z","shell.execute_reply":"2024-01-29T16:22:50.516085Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240129_162219-ufh3gh4a</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lorenzozanolin-52/uncategorized/runs/ufh3gh4a' target=\"_blank\">flowing-wildflower-1</a></strong> to <a href='https://wandb.ai/lorenzozanolin-52/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lorenzozanolin-52/uncategorized' target=\"_blank\">https://wandb.ai/lorenzozanolin-52/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lorenzozanolin-52/uncategorized/runs/ufh3gh4a' target=\"_blank\">https://wandb.ai/lorenzozanolin-52/uncategorized/runs/ufh3gh4a</a>"},"metadata":{}},{"name":"stdout","text":"DataParallel(\n  (module): ManyToOneRNN(\n    (embedding): Embedding(2002, 256)\n    (rnn): RNN(256, 128, batch_first=True)\n    (fc): Linear(in_features=128, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the training loop.","metadata":{}},{"cell_type":"code","source":"lr = 0.00001\no = 'a'\n\nif o == 'a':\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)\n    wandb.log({'optimizer':'Adam'})\nelif o == 'r':\n    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, alpha=0.9)\n    wandb.log({'optimizer':'RMSprop'})\n\ncriterion = nn.BCEWithLogitsLoss()\n\nnum_epochs = 200\n\nwandb.log({'lr': lr})\nwandb.log({'epochs': num_epochs})\n\nfor epoch in range(num_epochs):\n    # Set the model to training mode\n    model.train()\n    # Variable for total loss in each epoch\n    total_loss = 0.0\n    \n    # Iterate through the training data\n    for inputs, labels in train_loader:\n        inputs = inputs.to(device)  #move data on the GPU\n        labels = labels.to(device)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Reshape the labels\n        labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n            \n        # Compute the loss\n        loss = criterion(outputs, labels.float())  # Convert labels to float\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Update the total loss\n        total_loss += loss.item()\n    \n    # Calculate the average loss per epoch\n    Training_Loss = total_loss / len(train_loader)\n    \n    # Print the average loss per epoch during training\n    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {Training_Loss:.4f}')\n    \n    # Set the model to evaluation mode\n    model.eval()\n    \n    # Variables for total loss and number of correct predictions\n    total_loss = 0.0\n    correct_predictions = 0\n\n    # Iterate through the validation data\n    with torch.no_grad():  # Disable gradient computation during evaluation\n        for inputs, labels in valid_loader:\n            inputs = inputs.to(device)  #move data on the GPU\n            labels = labels.to(device)\n            \n            # Forward pass\n            \n            outputs = model(inputs)\n            \n            # Reshape the labels\n            labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n                \n            # Compute the loss\n            loss = criterion(outputs, labels.float())  # Convert labels to float\n            \n            # Update the total loss\n            total_loss += loss.item()\n            \n            # Calculate the number of correct predictions\n            threshold = 0.5\n            predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n            correct_predictions += (predicted_labels == labels.float()).sum().item()\n\n    \n    # Calculate the average loss per epoch during evaluation\n    average_loss = total_loss / len(valid_loader)\n    \n    # Calculate accuracy\n    accuracy = correct_predictions / len(valid_loader.dataset)\n    \n    # Print evaluation metrics\n    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n    \n    # Log metrics using WandB\n    wandb.log({\"Epoch\": epoch+1,\"Training Loss\": Training_Loss, \"Validation Loss\": average_loss, \"Validation Accuracy\": accuracy})","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:22:53.804630Z","iopub.execute_input":"2024-01-29T16:22:53.804987Z","iopub.status.idle":"2024-01-29T16:34:33.822536Z","shell.execute_reply.started":"2024-01-29T16:22:53.804958Z","shell.execute_reply":"2024-01-29T16:34:33.821630Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch [1/300], Training Loss: 0.6981\nEpoch [1/300], Validation Loss: 0.6951, Validation Accuracy: 0.5158\nEpoch [2/300], Training Loss: 0.6936\nEpoch [2/300], Validation Loss: 0.6933, Validation Accuracy: 0.5210\nEpoch [3/300], Training Loss: 0.6917\nEpoch [3/300], Validation Loss: 0.6918, Validation Accuracy: 0.5243\nEpoch [4/300], Training Loss: 0.6902\nEpoch [4/300], Validation Loss: 0.6908, Validation Accuracy: 0.5267\nEpoch [5/300], Training Loss: 0.6889\nEpoch [5/300], Validation Loss: 0.6899, Validation Accuracy: 0.5349\nEpoch [6/300], Training Loss: 0.6875\nEpoch [6/300], Validation Loss: 0.6890, Validation Accuracy: 0.5373\nEpoch [7/300], Training Loss: 0.6861\nEpoch [7/300], Validation Loss: 0.6882, Validation Accuracy: 0.5384\nEpoch [8/300], Training Loss: 0.6846\nEpoch [8/300], Validation Loss: 0.6871, Validation Accuracy: 0.5429\nEpoch [9/300], Training Loss: 0.6831\nEpoch [9/300], Validation Loss: 0.6861, Validation Accuracy: 0.5427\nEpoch [10/300], Training Loss: 0.6815\nEpoch [10/300], Validation Loss: 0.6849, Validation Accuracy: 0.5424\nEpoch [11/300], Training Loss: 0.6797\nEpoch [11/300], Validation Loss: 0.6835, Validation Accuracy: 0.5530\nEpoch [12/300], Training Loss: 0.6778\nEpoch [12/300], Validation Loss: 0.6820, Validation Accuracy: 0.5554\nEpoch [13/300], Training Loss: 0.6756\nEpoch [13/300], Validation Loss: 0.6802, Validation Accuracy: 0.5576\nEpoch [14/300], Training Loss: 0.6732\nEpoch [14/300], Validation Loss: 0.6781, Validation Accuracy: 0.5608\nEpoch [15/300], Training Loss: 0.6703\nEpoch [15/300], Validation Loss: 0.6752, Validation Accuracy: 0.5691\nEpoch [16/300], Training Loss: 0.6668\nEpoch [16/300], Validation Loss: 0.6715, Validation Accuracy: 0.5760\nEpoch [17/300], Training Loss: 0.6622\nEpoch [17/300], Validation Loss: 0.6662, Validation Accuracy: 0.5904\nEpoch [18/300], Training Loss: 0.6554\nEpoch [18/300], Validation Loss: 0.6569, Validation Accuracy: 0.6043\nEpoch [19/300], Training Loss: 0.6415\nEpoch [19/300], Validation Loss: 0.6315, Validation Accuracy: 0.6462\nEpoch [20/300], Training Loss: 0.5996\nEpoch [20/300], Validation Loss: 0.5872, Validation Accuracy: 0.6994\nEpoch [21/300], Training Loss: 0.5750\nEpoch [21/300], Validation Loss: 0.5797, Validation Accuracy: 0.7046\nEpoch [22/300], Training Loss: 0.5669\nEpoch [22/300], Validation Loss: 0.5751, Validation Accuracy: 0.7099\nEpoch [23/300], Training Loss: 0.5631\nEpoch [23/300], Validation Loss: 0.5681, Validation Accuracy: 0.7165\nEpoch [24/300], Training Loss: 0.5603\nEpoch [24/300], Validation Loss: 0.5623, Validation Accuracy: 0.7230\nEpoch [25/300], Training Loss: 0.5536\nEpoch [25/300], Validation Loss: 0.5595, Validation Accuracy: 0.7226\nEpoch [26/300], Training Loss: 0.5509\nEpoch [26/300], Validation Loss: 0.5566, Validation Accuracy: 0.7290\nEpoch [27/300], Training Loss: 0.5520\nEpoch [27/300], Validation Loss: 0.5553, Validation Accuracy: 0.7270\nEpoch [28/300], Training Loss: 0.5445\nEpoch [28/300], Validation Loss: 0.5538, Validation Accuracy: 0.7266\nEpoch [29/300], Training Loss: 0.5419\nEpoch [29/300], Validation Loss: 0.5514, Validation Accuracy: 0.7294\nEpoch [30/300], Training Loss: 0.5388\nEpoch [30/300], Validation Loss: 0.5469, Validation Accuracy: 0.7328\nEpoch [31/300], Training Loss: 0.5359\nEpoch [31/300], Validation Loss: 0.5434, Validation Accuracy: 0.7347\nEpoch [32/300], Training Loss: 0.5339\nEpoch [32/300], Validation Loss: 0.5420, Validation Accuracy: 0.7371\nEpoch [33/300], Training Loss: 0.5319\nEpoch [33/300], Validation Loss: 0.5411, Validation Accuracy: 0.7397\nEpoch [34/300], Training Loss: 0.5281\nEpoch [34/300], Validation Loss: 0.5357, Validation Accuracy: 0.7448\nEpoch [35/300], Training Loss: 0.5247\nEpoch [35/300], Validation Loss: 0.5340, Validation Accuracy: 0.7450\nEpoch [36/300], Training Loss: 0.5218\nEpoch [36/300], Validation Loss: 0.5306, Validation Accuracy: 0.7483\nEpoch [37/300], Training Loss: 0.5203\nEpoch [37/300], Validation Loss: 0.5288, Validation Accuracy: 0.7486\nEpoch [38/300], Training Loss: 0.5158\nEpoch [38/300], Validation Loss: 0.5268, Validation Accuracy: 0.7486\nEpoch [39/300], Training Loss: 0.5138\nEpoch [39/300], Validation Loss: 0.5245, Validation Accuracy: 0.7523\nEpoch [40/300], Training Loss: 0.5107\nEpoch [40/300], Validation Loss: 0.5224, Validation Accuracy: 0.7531\nEpoch [41/300], Training Loss: 0.5088\nEpoch [41/300], Validation Loss: 0.5195, Validation Accuracy: 0.7539\nEpoch [42/300], Training Loss: 0.5057\nEpoch [42/300], Validation Loss: 0.5197, Validation Accuracy: 0.7552\nEpoch [43/300], Training Loss: 0.5028\nEpoch [43/300], Validation Loss: 0.5148, Validation Accuracy: 0.7578\nEpoch [44/300], Training Loss: 0.5001\nEpoch [44/300], Validation Loss: 0.5131, Validation Accuracy: 0.7595\nEpoch [45/300], Training Loss: 0.4984\nEpoch [45/300], Validation Loss: 0.5109, Validation Accuracy: 0.7627\nEpoch [46/300], Training Loss: 0.4951\nEpoch [46/300], Validation Loss: 0.5085, Validation Accuracy: 0.7627\nEpoch [47/300], Training Loss: 0.4923\nEpoch [47/300], Validation Loss: 0.5064, Validation Accuracy: 0.7645\nEpoch [48/300], Training Loss: 0.4900\nEpoch [48/300], Validation Loss: 0.5061, Validation Accuracy: 0.7629\nEpoch [49/300], Training Loss: 0.4896\nEpoch [49/300], Validation Loss: 0.5036, Validation Accuracy: 0.7661\nEpoch [50/300], Training Loss: 0.4857\nEpoch [50/300], Validation Loss: 0.5016, Validation Accuracy: 0.7661\nEpoch [51/300], Training Loss: 0.4833\nEpoch [51/300], Validation Loss: 0.5017, Validation Accuracy: 0.7653\nEpoch [52/300], Training Loss: 0.4810\nEpoch [52/300], Validation Loss: 0.5001, Validation Accuracy: 0.7699\nEpoch [53/300], Training Loss: 0.4787\nEpoch [53/300], Validation Loss: 0.4951, Validation Accuracy: 0.7720\nEpoch [54/300], Training Loss: 0.4762\nEpoch [54/300], Validation Loss: 0.4963, Validation Accuracy: 0.7683\nEpoch [55/300], Training Loss: 0.4731\nEpoch [55/300], Validation Loss: 0.4920, Validation Accuracy: 0.7717\nEpoch [56/300], Training Loss: 0.4711\nEpoch [56/300], Validation Loss: 0.4908, Validation Accuracy: 0.7746\nEpoch [57/300], Training Loss: 0.4687\nEpoch [57/300], Validation Loss: 0.4918, Validation Accuracy: 0.7734\nEpoch [58/300], Training Loss: 0.4664\nEpoch [58/300], Validation Loss: 0.4888, Validation Accuracy: 0.7746\nEpoch [59/300], Training Loss: 0.4636\nEpoch [59/300], Validation Loss: 0.4861, Validation Accuracy: 0.7770\nEpoch [60/300], Training Loss: 0.4621\nEpoch [60/300], Validation Loss: 0.4837, Validation Accuracy: 0.7758\nEpoch [61/300], Training Loss: 0.4593\nEpoch [61/300], Validation Loss: 0.4820, Validation Accuracy: 0.7770\nEpoch [62/300], Training Loss: 0.4580\nEpoch [62/300], Validation Loss: 0.4807, Validation Accuracy: 0.7778\nEpoch [63/300], Training Loss: 0.4551\nEpoch [63/300], Validation Loss: 0.4809, Validation Accuracy: 0.7768\nEpoch [64/300], Training Loss: 0.4529\nEpoch [64/300], Validation Loss: 0.4785, Validation Accuracy: 0.7782\nEpoch [65/300], Training Loss: 0.4504\nEpoch [65/300], Validation Loss: 0.4777, Validation Accuracy: 0.7797\nEpoch [66/300], Training Loss: 0.4487\nEpoch [66/300], Validation Loss: 0.4764, Validation Accuracy: 0.7830\nEpoch [67/300], Training Loss: 0.4482\nEpoch [67/300], Validation Loss: 0.4734, Validation Accuracy: 0.7821\nEpoch [68/300], Training Loss: 0.4443\nEpoch [68/300], Validation Loss: 0.4741, Validation Accuracy: 0.7856\nEpoch [69/300], Training Loss: 0.4423\nEpoch [69/300], Validation Loss: 0.4707, Validation Accuracy: 0.7848\nEpoch [70/300], Training Loss: 0.4403\nEpoch [70/300], Validation Loss: 0.4773, Validation Accuracy: 0.7776\nEpoch [71/300], Training Loss: 0.4391\nEpoch [71/300], Validation Loss: 0.4691, Validation Accuracy: 0.7870\nEpoch [72/300], Training Loss: 0.4370\nEpoch [72/300], Validation Loss: 0.4684, Validation Accuracy: 0.7845\nEpoch [73/300], Training Loss: 0.4350\nEpoch [73/300], Validation Loss: 0.4669, Validation Accuracy: 0.7867\nEpoch [74/300], Training Loss: 0.4340\nEpoch [74/300], Validation Loss: 0.4648, Validation Accuracy: 0.7877\nEpoch [75/300], Training Loss: 0.4311\nEpoch [75/300], Validation Loss: 0.4616, Validation Accuracy: 0.7898\nEpoch [76/300], Training Loss: 0.4297\nEpoch [76/300], Validation Loss: 0.4659, Validation Accuracy: 0.7834\nEpoch [77/300], Training Loss: 0.4276\nEpoch [77/300], Validation Loss: 0.4587, Validation Accuracy: 0.7906\nEpoch [78/300], Training Loss: 0.4265\nEpoch [78/300], Validation Loss: 0.4571, Validation Accuracy: 0.7934\nEpoch [79/300], Training Loss: 0.4247\nEpoch [79/300], Validation Loss: 0.4606, Validation Accuracy: 0.7958\nEpoch [80/300], Training Loss: 0.4236\nEpoch [80/300], Validation Loss: 0.4534, Validation Accuracy: 0.7939\nEpoch [81/300], Training Loss: 0.4204\nEpoch [81/300], Validation Loss: 0.4531, Validation Accuracy: 0.7926\nEpoch [82/300], Training Loss: 0.4189\nEpoch [82/300], Validation Loss: 0.4512, Validation Accuracy: 0.7950\nEpoch [83/300], Training Loss: 0.4170\nEpoch [83/300], Validation Loss: 0.4493, Validation Accuracy: 0.7957\nEpoch [84/300], Training Loss: 0.4161\nEpoch [84/300], Validation Loss: 0.4503, Validation Accuracy: 0.7966\nEpoch [85/300], Training Loss: 0.4146\nEpoch [85/300], Validation Loss: 0.4458, Validation Accuracy: 0.7954\nEpoch [86/300], Training Loss: 0.4123\nEpoch [86/300], Validation Loss: 0.4460, Validation Accuracy: 0.7970\nEpoch [87/300], Training Loss: 0.4100\nEpoch [87/300], Validation Loss: 0.4439, Validation Accuracy: 0.7970\nEpoch [88/300], Training Loss: 0.4086\nEpoch [88/300], Validation Loss: 0.4423, Validation Accuracy: 0.7997\nEpoch [89/300], Training Loss: 0.4063\nEpoch [89/300], Validation Loss: 0.4425, Validation Accuracy: 0.7998\nEpoch [90/300], Training Loss: 0.4045\nEpoch [90/300], Validation Loss: 0.4418, Validation Accuracy: 0.7997\nEpoch [91/300], Training Loss: 0.4032\nEpoch [91/300], Validation Loss: 0.4393, Validation Accuracy: 0.7994\nEpoch [92/300], Training Loss: 0.4020\nEpoch [92/300], Validation Loss: 0.4404, Validation Accuracy: 0.8005\nEpoch [93/300], Training Loss: 0.3998\nEpoch [93/300], Validation Loss: 0.4383, Validation Accuracy: 0.8032\nEpoch [94/300], Training Loss: 0.3985\nEpoch [94/300], Validation Loss: 0.4378, Validation Accuracy: 0.8029\nEpoch [95/300], Training Loss: 0.3980\nEpoch [95/300], Validation Loss: 0.4382, Validation Accuracy: 0.7995\nEpoch [96/300], Training Loss: 0.3947\nEpoch [96/300], Validation Loss: 0.4370, Validation Accuracy: 0.8058\nEpoch [97/300], Training Loss: 0.3935\nEpoch [97/300], Validation Loss: 0.4313, Validation Accuracy: 0.8059\nEpoch [98/300], Training Loss: 0.3927\nEpoch [98/300], Validation Loss: 0.4308, Validation Accuracy: 0.8077\nEpoch [99/300], Training Loss: 0.3909\nEpoch [99/300], Validation Loss: 0.4290, Validation Accuracy: 0.8054\nEpoch [100/300], Training Loss: 0.3885\nEpoch [100/300], Validation Loss: 0.4291, Validation Accuracy: 0.8077\nEpoch [101/300], Training Loss: 0.3875\nEpoch [101/300], Validation Loss: 0.4280, Validation Accuracy: 0.8078\nEpoch [102/300], Training Loss: 0.3869\nEpoch [102/300], Validation Loss: 0.4275, Validation Accuracy: 0.8080\nEpoch [103/300], Training Loss: 0.3847\nEpoch [103/300], Validation Loss: 0.4296, Validation Accuracy: 0.8080\nEpoch [104/300], Training Loss: 0.3827\nEpoch [104/300], Validation Loss: 0.4257, Validation Accuracy: 0.8109\nEpoch [105/300], Training Loss: 0.3815\nEpoch [105/300], Validation Loss: 0.4252, Validation Accuracy: 0.8110\nEpoch [106/300], Training Loss: 0.3797\nEpoch [106/300], Validation Loss: 0.4229, Validation Accuracy: 0.8118\nEpoch [107/300], Training Loss: 0.3786\nEpoch [107/300], Validation Loss: 0.4215, Validation Accuracy: 0.8114\nEpoch [108/300], Training Loss: 0.3784\nEpoch [108/300], Validation Loss: 0.4234, Validation Accuracy: 0.8122\nEpoch [109/300], Training Loss: 0.3760\nEpoch [109/300], Validation Loss: 0.4281, Validation Accuracy: 0.8136\nEpoch [110/300], Training Loss: 0.3749\nEpoch [110/300], Validation Loss: 0.4188, Validation Accuracy: 0.8133\nEpoch [111/300], Training Loss: 0.3727\nEpoch [111/300], Validation Loss: 0.4206, Validation Accuracy: 0.8139\nEpoch [112/300], Training Loss: 0.3710\nEpoch [112/300], Validation Loss: 0.4176, Validation Accuracy: 0.8158\nEpoch [113/300], Training Loss: 0.3707\nEpoch [113/300], Validation Loss: 0.4180, Validation Accuracy: 0.8152\nEpoch [114/300], Training Loss: 0.3681\nEpoch [114/300], Validation Loss: 0.4164, Validation Accuracy: 0.8152\nEpoch [115/300], Training Loss: 0.3668\nEpoch [115/300], Validation Loss: 0.4168, Validation Accuracy: 0.8154\nEpoch [116/300], Training Loss: 0.3653\nEpoch [116/300], Validation Loss: 0.4166, Validation Accuracy: 0.8136\nEpoch [117/300], Training Loss: 0.3640\nEpoch [117/300], Validation Loss: 0.4176, Validation Accuracy: 0.8131\nEpoch [118/300], Training Loss: 0.3639\nEpoch [118/300], Validation Loss: 0.4111, Validation Accuracy: 0.8171\nEpoch [119/300], Training Loss: 0.3616\nEpoch [119/300], Validation Loss: 0.4139, Validation Accuracy: 0.8190\nEpoch [120/300], Training Loss: 0.3602\nEpoch [120/300], Validation Loss: 0.4137, Validation Accuracy: 0.8192\nEpoch [121/300], Training Loss: 0.3597\nEpoch [121/300], Validation Loss: 0.4120, Validation Accuracy: 0.8210\nEpoch [122/300], Training Loss: 0.3578\nEpoch [122/300], Validation Loss: 0.4130, Validation Accuracy: 0.8157\nEpoch [123/300], Training Loss: 0.3568\nEpoch [123/300], Validation Loss: 0.4108, Validation Accuracy: 0.8186\nEpoch [124/300], Training Loss: 0.3552\nEpoch [124/300], Validation Loss: 0.4081, Validation Accuracy: 0.8208\nEpoch [125/300], Training Loss: 0.3539\nEpoch [125/300], Validation Loss: 0.4079, Validation Accuracy: 0.8205\nEpoch [126/300], Training Loss: 0.3528\nEpoch [126/300], Validation Loss: 0.4055, Validation Accuracy: 0.8224\nEpoch [127/300], Training Loss: 0.3516\nEpoch [127/300], Validation Loss: 0.4056, Validation Accuracy: 0.8224\nEpoch [128/300], Training Loss: 0.3517\nEpoch [128/300], Validation Loss: 0.4047, Validation Accuracy: 0.8211\nEpoch [129/300], Training Loss: 0.3505\nEpoch [129/300], Validation Loss: 0.4045, Validation Accuracy: 0.8226\nEpoch [130/300], Training Loss: 0.3483\nEpoch [130/300], Validation Loss: 0.4042, Validation Accuracy: 0.8218\nEpoch [131/300], Training Loss: 0.3469\nEpoch [131/300], Validation Loss: 0.4043, Validation Accuracy: 0.8226\nEpoch [132/300], Training Loss: 0.3463\nEpoch [132/300], Validation Loss: 0.4052, Validation Accuracy: 0.8242\nEpoch [133/300], Training Loss: 0.3442\nEpoch [133/300], Validation Loss: 0.4026, Validation Accuracy: 0.8240\nEpoch [134/300], Training Loss: 0.3432\nEpoch [134/300], Validation Loss: 0.4008, Validation Accuracy: 0.8242\nEpoch [135/300], Training Loss: 0.3421\nEpoch [135/300], Validation Loss: 0.4002, Validation Accuracy: 0.8248\nEpoch [136/300], Training Loss: 0.3418\nEpoch [136/300], Validation Loss: 0.4010, Validation Accuracy: 0.8251\nEpoch [137/300], Training Loss: 0.3412\nEpoch [137/300], Validation Loss: 0.3995, Validation Accuracy: 0.8269\nEpoch [138/300], Training Loss: 0.3391\nEpoch [138/300], Validation Loss: 0.3972, Validation Accuracy: 0.8256\nEpoch [139/300], Training Loss: 0.3387\nEpoch [139/300], Validation Loss: 0.3958, Validation Accuracy: 0.8277\nEpoch [140/300], Training Loss: 0.3371\nEpoch [140/300], Validation Loss: 0.4061, Validation Accuracy: 0.8250\nEpoch [141/300], Training Loss: 0.3368\nEpoch [141/300], Validation Loss: 0.3991, Validation Accuracy: 0.8283\nEpoch [142/300], Training Loss: 0.3345\nEpoch [142/300], Validation Loss: 0.3978, Validation Accuracy: 0.8286\nEpoch [143/300], Training Loss: 0.3326\nEpoch [143/300], Validation Loss: 0.3970, Validation Accuracy: 0.8288\nEpoch [144/300], Training Loss: 0.3325\nEpoch [144/300], Validation Loss: 0.3952, Validation Accuracy: 0.8312\nEpoch [145/300], Training Loss: 0.3313\nEpoch [145/300], Validation Loss: 0.4011, Validation Accuracy: 0.8269\nEpoch [146/300], Training Loss: 0.3303\nEpoch [146/300], Validation Loss: 0.3997, Validation Accuracy: 0.8285\nEpoch [147/300], Training Loss: 0.3292\nEpoch [147/300], Validation Loss: 0.3998, Validation Accuracy: 0.8262\nEpoch [148/300], Training Loss: 0.3293\nEpoch [148/300], Validation Loss: 0.3945, Validation Accuracy: 0.8304\nEpoch [149/300], Training Loss: 0.3269\nEpoch [149/300], Validation Loss: 0.3919, Validation Accuracy: 0.8318\nEpoch [150/300], Training Loss: 0.3261\nEpoch [150/300], Validation Loss: 0.3888, Validation Accuracy: 0.8310\nEpoch [151/300], Training Loss: 0.3265\nEpoch [151/300], Validation Loss: 0.3954, Validation Accuracy: 0.8310\nEpoch [152/300], Training Loss: 0.3252\nEpoch [152/300], Validation Loss: 0.3948, Validation Accuracy: 0.8309\nEpoch [153/300], Training Loss: 0.3230\nEpoch [153/300], Validation Loss: 0.3955, Validation Accuracy: 0.8304\nEpoch [154/300], Training Loss: 0.3226\nEpoch [154/300], Validation Loss: 0.3972, Validation Accuracy: 0.8302\nEpoch [155/300], Training Loss: 0.3218\nEpoch [155/300], Validation Loss: 0.3939, Validation Accuracy: 0.8317\nEpoch [156/300], Training Loss: 0.3205\nEpoch [156/300], Validation Loss: 0.3971, Validation Accuracy: 0.8317\nEpoch [157/300], Training Loss: 0.3197\nEpoch [157/300], Validation Loss: 0.3923, Validation Accuracy: 0.8320\nEpoch [158/300], Training Loss: 0.3202\nEpoch [158/300], Validation Loss: 0.3892, Validation Accuracy: 0.8334\nEpoch [159/300], Training Loss: 0.3191\nEpoch [159/300], Validation Loss: 0.3921, Validation Accuracy: 0.8341\nEpoch [160/300], Training Loss: 0.3172\nEpoch [160/300], Validation Loss: 0.3876, Validation Accuracy: 0.8325\nEpoch [161/300], Training Loss: 0.3160\nEpoch [161/300], Validation Loss: 0.3929, Validation Accuracy: 0.8322\nEpoch [162/300], Training Loss: 0.3155\nEpoch [162/300], Validation Loss: 0.3934, Validation Accuracy: 0.8331\nEpoch [163/300], Training Loss: 0.3172\nEpoch [163/300], Validation Loss: 0.3870, Validation Accuracy: 0.8352\nEpoch [164/300], Training Loss: 0.3137\nEpoch [164/300], Validation Loss: 0.3933, Validation Accuracy: 0.8304\nEpoch [165/300], Training Loss: 0.3140\nEpoch [165/300], Validation Loss: 0.3887, Validation Accuracy: 0.8360\nEpoch [166/300], Training Loss: 0.3119\nEpoch [166/300], Validation Loss: 0.3902, Validation Accuracy: 0.8350\nEpoch [167/300], Training Loss: 0.3121\nEpoch [167/300], Validation Loss: 0.3927, Validation Accuracy: 0.8325\nEpoch [168/300], Training Loss: 0.3103\nEpoch [168/300], Validation Loss: 0.3848, Validation Accuracy: 0.8382\nEpoch [169/300], Training Loss: 0.3097\nEpoch [169/300], Validation Loss: 0.3834, Validation Accuracy: 0.8378\nEpoch [170/300], Training Loss: 0.3083\nEpoch [170/300], Validation Loss: 0.3841, Validation Accuracy: 0.8358\nEpoch [171/300], Training Loss: 0.3075\nEpoch [171/300], Validation Loss: 0.3861, Validation Accuracy: 0.8394\nEpoch [172/300], Training Loss: 0.3064\nEpoch [172/300], Validation Loss: 0.3863, Validation Accuracy: 0.8349\nEpoch [173/300], Training Loss: 0.3073\nEpoch [173/300], Validation Loss: 0.3933, Validation Accuracy: 0.8338\nEpoch [174/300], Training Loss: 0.3055\nEpoch [174/300], Validation Loss: 0.3813, Validation Accuracy: 0.8349\nEpoch [175/300], Training Loss: 0.3041\nEpoch [175/300], Validation Loss: 0.3842, Validation Accuracy: 0.8355\nEpoch [176/300], Training Loss: 0.3031\nEpoch [176/300], Validation Loss: 0.3846, Validation Accuracy: 0.8368\nEpoch [177/300], Training Loss: 0.3033\nEpoch [177/300], Validation Loss: 0.3852, Validation Accuracy: 0.8378\nEpoch [178/300], Training Loss: 0.3019\nEpoch [178/300], Validation Loss: 0.3874, Validation Accuracy: 0.8346\nEpoch [179/300], Training Loss: 0.3010\nEpoch [179/300], Validation Loss: 0.3867, Validation Accuracy: 0.8374\nEpoch [180/300], Training Loss: 0.2995\nEpoch [180/300], Validation Loss: 0.3841, Validation Accuracy: 0.8408\nEpoch [181/300], Training Loss: 0.3007\nEpoch [181/300], Validation Loss: 0.3947, Validation Accuracy: 0.8344\nEpoch [182/300], Training Loss: 0.2990\nEpoch [182/300], Validation Loss: 0.3852, Validation Accuracy: 0.8402\nEpoch [183/300], Training Loss: 0.2990\nEpoch [183/300], Validation Loss: 0.3842, Validation Accuracy: 0.8405\nEpoch [184/300], Training Loss: 0.2962\nEpoch [184/300], Validation Loss: 0.3846, Validation Accuracy: 0.8398\nEpoch [185/300], Training Loss: 0.2957\nEpoch [185/300], Validation Loss: 0.3839, Validation Accuracy: 0.8405\nEpoch [186/300], Training Loss: 0.2957\nEpoch [186/300], Validation Loss: 0.3849, Validation Accuracy: 0.8382\nEpoch [187/300], Training Loss: 0.2947\nEpoch [187/300], Validation Loss: 0.3825, Validation Accuracy: 0.8394\nEpoch [188/300], Training Loss: 0.2955\nEpoch [188/300], Validation Loss: 0.3881, Validation Accuracy: 0.8387\nEpoch [189/300], Training Loss: 0.2944\nEpoch [189/300], Validation Loss: 0.3873, Validation Accuracy: 0.8394\nEpoch [190/300], Training Loss: 0.2924\nEpoch [190/300], Validation Loss: 0.3819, Validation Accuracy: 0.8389\nEpoch [191/300], Training Loss: 0.2926\nEpoch [191/300], Validation Loss: 0.3833, Validation Accuracy: 0.8405\nEpoch [192/300], Training Loss: 0.2900\nEpoch [192/300], Validation Loss: 0.3827, Validation Accuracy: 0.8395\nEpoch [193/300], Training Loss: 0.2897\nEpoch [193/300], Validation Loss: 0.3881, Validation Accuracy: 0.8386\nEpoch [194/300], Training Loss: 0.2890\nEpoch [194/300], Validation Loss: 0.3790, Validation Accuracy: 0.8395\nEpoch [195/300], Training Loss: 0.2883\nEpoch [195/300], Validation Loss: 0.3856, Validation Accuracy: 0.8398\nEpoch [196/300], Training Loss: 0.2882\nEpoch [196/300], Validation Loss: 0.3851, Validation Accuracy: 0.8413\nEpoch [197/300], Training Loss: 0.2876\nEpoch [197/300], Validation Loss: 0.3803, Validation Accuracy: 0.8410\nEpoch [198/300], Training Loss: 0.2862\nEpoch [198/300], Validation Loss: 0.3836, Validation Accuracy: 0.8390\nEpoch [199/300], Training Loss: 0.2869\nEpoch [199/300], Validation Loss: 0.3864, Validation Accuracy: 0.8402\nEpoch [200/300], Training Loss: 0.2853\nEpoch [200/300], Validation Loss: 0.3874, Validation Accuracy: 0.8398\nEpoch [201/300], Training Loss: 0.2837\nEpoch [201/300], Validation Loss: 0.3844, Validation Accuracy: 0.8414\nEpoch [202/300], Training Loss: 0.2854\nEpoch [202/300], Validation Loss: 0.3819, Validation Accuracy: 0.8418\nEpoch [203/300], Training Loss: 0.2830\nEpoch [203/300], Validation Loss: 0.3781, Validation Accuracy: 0.8410\nEpoch [204/300], Training Loss: 0.2814\nEpoch [204/300], Validation Loss: 0.3842, Validation Accuracy: 0.8418\nEpoch [205/300], Training Loss: 0.2827\nEpoch [205/300], Validation Loss: 0.3847, Validation Accuracy: 0.8421\nEpoch [206/300], Training Loss: 0.2800\nEpoch [206/300], Validation Loss: 0.3850, Validation Accuracy: 0.8426\nEpoch [207/300], Training Loss: 0.2800\nEpoch [207/300], Validation Loss: 0.3902, Validation Accuracy: 0.8410\nEpoch [208/300], Training Loss: 0.2789\nEpoch [208/300], Validation Loss: 0.3912, Validation Accuracy: 0.8405\nEpoch [209/300], Training Loss: 0.2796\nEpoch [209/300], Validation Loss: 0.3872, Validation Accuracy: 0.8365\nEpoch [210/300], Training Loss: 0.2771\nEpoch [210/300], Validation Loss: 0.3879, Validation Accuracy: 0.8421\nEpoch [211/300], Training Loss: 0.2774\nEpoch [211/300], Validation Loss: 0.3798, Validation Accuracy: 0.8434\nEpoch [212/300], Training Loss: 0.2760\nEpoch [212/300], Validation Loss: 0.3827, Validation Accuracy: 0.8434\nEpoch [213/300], Training Loss: 0.2757\nEpoch [213/300], Validation Loss: 0.3826, Validation Accuracy: 0.8430\nEpoch [214/300], Training Loss: 0.2740\nEpoch [214/300], Validation Loss: 0.3800, Validation Accuracy: 0.8405\nEpoch [215/300], Training Loss: 0.2735\nEpoch [215/300], Validation Loss: 0.3803, Validation Accuracy: 0.8426\nEpoch [216/300], Training Loss: 0.2744\nEpoch [216/300], Validation Loss: 0.3828, Validation Accuracy: 0.8427\nEpoch [217/300], Training Loss: 0.2768\nEpoch [217/300], Validation Loss: 0.3777, Validation Accuracy: 0.8427\nEpoch [218/300], Training Loss: 0.2717\nEpoch [218/300], Validation Loss: 0.3845, Validation Accuracy: 0.8438\nEpoch [219/300], Training Loss: 0.2711\nEpoch [219/300], Validation Loss: 0.3824, Validation Accuracy: 0.8429\nEpoch [220/300], Training Loss: 0.2703\nEpoch [220/300], Validation Loss: 0.3831, Validation Accuracy: 0.8442\nEpoch [221/300], Training Loss: 0.2709\nEpoch [221/300], Validation Loss: 0.3883, Validation Accuracy: 0.8430\nEpoch [222/300], Training Loss: 0.2692\nEpoch [222/300], Validation Loss: 0.3847, Validation Accuracy: 0.8422\nEpoch [223/300], Training Loss: 0.2687\nEpoch [223/300], Validation Loss: 0.3965, Validation Accuracy: 0.8413\nEpoch [224/300], Training Loss: 0.2701\nEpoch [224/300], Validation Loss: 0.3874, Validation Accuracy: 0.8438\nEpoch [225/300], Training Loss: 0.2671\nEpoch [225/300], Validation Loss: 0.3838, Validation Accuracy: 0.8430\nEpoch [226/300], Training Loss: 0.2669\nEpoch [226/300], Validation Loss: 0.3827, Validation Accuracy: 0.8438\nEpoch [227/300], Training Loss: 0.2666\nEpoch [227/300], Validation Loss: 0.3861, Validation Accuracy: 0.8430\nEpoch [228/300], Training Loss: 0.2647\nEpoch [228/300], Validation Loss: 0.3798, Validation Accuracy: 0.8440\nEpoch [229/300], Training Loss: 0.2645\nEpoch [229/300], Validation Loss: 0.3894, Validation Accuracy: 0.8429\nEpoch [230/300], Training Loss: 0.2644\nEpoch [230/300], Validation Loss: 0.3947, Validation Accuracy: 0.8426\nEpoch [231/300], Training Loss: 0.2640\nEpoch [231/300], Validation Loss: 0.3830, Validation Accuracy: 0.8434\nEpoch [232/300], Training Loss: 0.2631\nEpoch [232/300], Validation Loss: 0.3843, Validation Accuracy: 0.8434\nEpoch [233/300], Training Loss: 0.2623\nEpoch [233/300], Validation Loss: 0.3871, Validation Accuracy: 0.8392\nEpoch [234/300], Training Loss: 0.2616\nEpoch [234/300], Validation Loss: 0.3806, Validation Accuracy: 0.8437\nEpoch [235/300], Training Loss: 0.2608\nEpoch [235/300], Validation Loss: 0.3871, Validation Accuracy: 0.8442\nEpoch [236/300], Training Loss: 0.2606\nEpoch [236/300], Validation Loss: 0.3941, Validation Accuracy: 0.8427\nEpoch [237/300], Training Loss: 0.2601\nEpoch [237/300], Validation Loss: 0.3797, Validation Accuracy: 0.8445\nEpoch [238/300], Training Loss: 0.2586\nEpoch [238/300], Validation Loss: 0.3846, Validation Accuracy: 0.8430\nEpoch [239/300], Training Loss: 0.2584\nEpoch [239/300], Validation Loss: 0.3879, Validation Accuracy: 0.8445\nEpoch [240/300], Training Loss: 0.2581\nEpoch [240/300], Validation Loss: 0.3943, Validation Accuracy: 0.8419\nEpoch [241/300], Training Loss: 0.2578\nEpoch [241/300], Validation Loss: 0.3861, Validation Accuracy: 0.8434\nEpoch [242/300], Training Loss: 0.2561\nEpoch [242/300], Validation Loss: 0.3907, Validation Accuracy: 0.8445\nEpoch [243/300], Training Loss: 0.2567\nEpoch [243/300], Validation Loss: 0.3833, Validation Accuracy: 0.8435\nEpoch [244/300], Training Loss: 0.2559\nEpoch [244/300], Validation Loss: 0.3948, Validation Accuracy: 0.8414\nEpoch [245/300], Training Loss: 0.2552\nEpoch [245/300], Validation Loss: 0.3824, Validation Accuracy: 0.8446\nEpoch [246/300], Training Loss: 0.2543\nEpoch [246/300], Validation Loss: 0.3869, Validation Accuracy: 0.8437\nEpoch [247/300], Training Loss: 0.2538\nEpoch [247/300], Validation Loss: 0.3823, Validation Accuracy: 0.8440\nEpoch [248/300], Training Loss: 0.2528\nEpoch [248/300], Validation Loss: 0.3811, Validation Accuracy: 0.8429\nEpoch [249/300], Training Loss: 0.2532\nEpoch [249/300], Validation Loss: 0.3875, Validation Accuracy: 0.8430\nEpoch [250/300], Training Loss: 0.2512\nEpoch [250/300], Validation Loss: 0.3835, Validation Accuracy: 0.8448\nEpoch [251/300], Training Loss: 0.2526\nEpoch [251/300], Validation Loss: 0.3946, Validation Accuracy: 0.8430\nEpoch [252/300], Training Loss: 0.2506\nEpoch [252/300], Validation Loss: 0.3869, Validation Accuracy: 0.8416\nEpoch [253/300], Training Loss: 0.2504\nEpoch [253/300], Validation Loss: 0.3818, Validation Accuracy: 0.8434\nEpoch [254/300], Training Loss: 0.2504\nEpoch [254/300], Validation Loss: 0.3816, Validation Accuracy: 0.8432\nEpoch [255/300], Training Loss: 0.2483\nEpoch [255/300], Validation Loss: 0.3883, Validation Accuracy: 0.8434\nEpoch [256/300], Training Loss: 0.2503\nEpoch [256/300], Validation Loss: 0.3940, Validation Accuracy: 0.8450\nEpoch [257/300], Training Loss: 0.2470\nEpoch [257/300], Validation Loss: 0.3908, Validation Accuracy: 0.8438\nEpoch [258/300], Training Loss: 0.2469\nEpoch [258/300], Validation Loss: 0.3798, Validation Accuracy: 0.8424\nEpoch [259/300], Training Loss: 0.2463\nEpoch [259/300], Validation Loss: 0.3856, Validation Accuracy: 0.8430\nEpoch [260/300], Training Loss: 0.2470\nEpoch [260/300], Validation Loss: 0.3923, Validation Accuracy: 0.8450\nEpoch [261/300], Training Loss: 0.2455\nEpoch [261/300], Validation Loss: 0.3834, Validation Accuracy: 0.8446\nEpoch [262/300], Training Loss: 0.2451\nEpoch [262/300], Validation Loss: 0.3869, Validation Accuracy: 0.8434\nEpoch [263/300], Training Loss: 0.2449\nEpoch [263/300], Validation Loss: 0.3872, Validation Accuracy: 0.8438\nEpoch [264/300], Training Loss: 0.2431\nEpoch [264/300], Validation Loss: 0.3851, Validation Accuracy: 0.8430\nEpoch [265/300], Training Loss: 0.2423\nEpoch [265/300], Validation Loss: 0.3985, Validation Accuracy: 0.8443\nEpoch [266/300], Training Loss: 0.2421\nEpoch [266/300], Validation Loss: 0.3880, Validation Accuracy: 0.8454\nEpoch [267/300], Training Loss: 0.2433\nEpoch [267/300], Validation Loss: 0.3860, Validation Accuracy: 0.8446\nEpoch [268/300], Training Loss: 0.2416\nEpoch [268/300], Validation Loss: 0.3888, Validation Accuracy: 0.8448\nEpoch [269/300], Training Loss: 0.2428\nEpoch [269/300], Validation Loss: 0.3886, Validation Accuracy: 0.8454\nEpoch [270/300], Training Loss: 0.2401\nEpoch [270/300], Validation Loss: 0.3835, Validation Accuracy: 0.8445\nEpoch [271/300], Training Loss: 0.2392\nEpoch [271/300], Validation Loss: 0.3873, Validation Accuracy: 0.8440\nEpoch [272/300], Training Loss: 0.2402\nEpoch [272/300], Validation Loss: 0.3856, Validation Accuracy: 0.8474\nEpoch [273/300], Training Loss: 0.2383\nEpoch [273/300], Validation Loss: 0.3894, Validation Accuracy: 0.8442\nEpoch [274/300], Training Loss: 0.2373\nEpoch [274/300], Validation Loss: 0.3909, Validation Accuracy: 0.8459\nEpoch [275/300], Training Loss: 0.2376\nEpoch [275/300], Validation Loss: 0.3907, Validation Accuracy: 0.8464\nEpoch [276/300], Training Loss: 0.2362\nEpoch [276/300], Validation Loss: 0.3902, Validation Accuracy: 0.8461\nEpoch [277/300], Training Loss: 0.2364\nEpoch [277/300], Validation Loss: 0.3872, Validation Accuracy: 0.8453\nEpoch [278/300], Training Loss: 0.2347\nEpoch [278/300], Validation Loss: 0.3932, Validation Accuracy: 0.8451\nEpoch [279/300], Training Loss: 0.2351\nEpoch [279/300], Validation Loss: 0.3843, Validation Accuracy: 0.8450\nEpoch [280/300], Training Loss: 0.2352\nEpoch [280/300], Validation Loss: 0.4046, Validation Accuracy: 0.8453\nEpoch [281/300], Training Loss: 0.2340\nEpoch [281/300], Validation Loss: 0.3981, Validation Accuracy: 0.8453\nEpoch [282/300], Training Loss: 0.2337\nEpoch [282/300], Validation Loss: 0.3920, Validation Accuracy: 0.8443\nEpoch [283/300], Training Loss: 0.2327\nEpoch [283/300], Validation Loss: 0.3924, Validation Accuracy: 0.8435\nEpoch [284/300], Training Loss: 0.2333\nEpoch [284/300], Validation Loss: 0.3957, Validation Accuracy: 0.8451\nEpoch [285/300], Training Loss: 0.2314\nEpoch [285/300], Validation Loss: 0.3996, Validation Accuracy: 0.8453\nEpoch [286/300], Training Loss: 0.2308\nEpoch [286/300], Validation Loss: 0.3913, Validation Accuracy: 0.8459\nEpoch [287/300], Training Loss: 0.2317\nEpoch [287/300], Validation Loss: 0.4024, Validation Accuracy: 0.8395\nEpoch [288/300], Training Loss: 0.2299\nEpoch [288/300], Validation Loss: 0.3919, Validation Accuracy: 0.8458\nEpoch [289/300], Training Loss: 0.2285\nEpoch [289/300], Validation Loss: 0.3901, Validation Accuracy: 0.8461\nEpoch [290/300], Training Loss: 0.2291\nEpoch [290/300], Validation Loss: 0.3973, Validation Accuracy: 0.8422\nEpoch [291/300], Training Loss: 0.2278\nEpoch [291/300], Validation Loss: 0.3925, Validation Accuracy: 0.8448\nEpoch [292/300], Training Loss: 0.2286\nEpoch [292/300], Validation Loss: 0.3966, Validation Accuracy: 0.8454\nEpoch [293/300], Training Loss: 0.2280\nEpoch [293/300], Validation Loss: 0.3903, Validation Accuracy: 0.8464\nEpoch [294/300], Training Loss: 0.2279\nEpoch [294/300], Validation Loss: 0.3953, Validation Accuracy: 0.8469\nEpoch [295/300], Training Loss: 0.2265\nEpoch [295/300], Validation Loss: 0.3992, Validation Accuracy: 0.8453\nEpoch [296/300], Training Loss: 0.2245\nEpoch [296/300], Validation Loss: 0.3926, Validation Accuracy: 0.8464\nEpoch [297/300], Training Loss: 0.2253\nEpoch [297/300], Validation Loss: 0.3973, Validation Accuracy: 0.8469\nEpoch [298/300], Training Loss: 0.2250\nEpoch [298/300], Validation Loss: 0.3928, Validation Accuracy: 0.8443\nEpoch [299/300], Training Loss: 0.2233\nEpoch [299/300], Validation Loss: 0.3975, Validation Accuracy: 0.8470\nEpoch [300/300], Training Loss: 0.2235\nEpoch [300/300], Validation Loss: 0.3901, Validation Accuracy: 0.8475\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Variables for total loss and number of correct predictions\ntotal_loss = 0.0\ncorrect_predictions = 0\n\n# Iterate through the test data\nwith torch.no_grad():  # Disable gradient computation during evaluation\n    for inputs, labels in test_loader:\n        inputs = inputs.to(device)  #move data on the GPU\n        labels = labels.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Reshape the model's output\n        outputs = outputs.view(-1)  # Change dimensions from [batch_size, 1] to [batch_size]\n        \n        # Compute the loss\n        loss = criterion(outputs, labels.float())  # Convert labels to float\n        \n        # Update the total loss\n        total_loss += loss.item()\n        \n        # Calculate the number of correct predictions\n        threshold = 0.5\n        predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n        correct_predictions += (predicted_labels == labels.float()).sum().item()\n        \n# Calculate the average loss for the test set\naverage_loss = total_loss / len(test_loader)\n\n# Calculate accuracy on the test set\naccuracy = correct_predictions / len(test_loader.dataset)\n\nprint(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n\n# Log metrics using WandB\nwandb.log({\"Test Loss\": average_loss, \"Test Accuracy\": accuracy})\nwandb.finish()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T16:34:44.270079Z","iopub.execute_input":"2024-01-29T16:34:44.270834Z","iopub.status.idle":"2024-01-29T16:34:47.386179Z","shell.execute_reply.started":"2024-01-29T16:34:44.270798Z","shell.execute_reply":"2024-01-29T16:34:47.385502Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Test Loss: 0.4161, Test Accuracy: 0.8355\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.078 MB of 0.078 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Embedding Size</td><td>▁</td></tr><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>Training Loss</td><td>███▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▁▂▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>Validation Loss</td><td>███▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epochs</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Embedding Size</td><td>256</td></tr><tr><td>Epoch</td><td>300</td></tr><tr><td>Test Accuracy</td><td>0.83552</td></tr><tr><td>Test Loss</td><td>0.41607</td></tr><tr><td>Training Loss</td><td>0.22347</td></tr><tr><td>Validation Accuracy</td><td>0.84752</td></tr><tr><td>Validation Loss</td><td>0.39009</td></tr><tr><td>epochs</td><td>300</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>optimizer</td><td>Adam</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">flowing-wildflower-1</strong> at: <a href='https://wandb.ai/lorenzozanolin-52/uncategorized/runs/ufh3gh4a' target=\"_blank\">https://wandb.ai/lorenzozanolin-52/uncategorized/runs/ufh3gh4a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240129_162219-ufh3gh4a/logs</code>"},"metadata":{}}]}]}