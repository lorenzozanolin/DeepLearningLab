{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a52496e",
   "metadata": {
    "papermill": {
     "duration": 0.009721,
     "end_time": "2024-01-29T18:44:52.231053",
     "exception": false,
     "start_time": "2024-01-29T18:44:52.221332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMDB sentiment analysis with RNNs\n",
    "\n",
    "Kaggle: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0373cd4d",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:44:52.252090Z",
     "iopub.status.busy": "2024-01-29T18:44:52.251328Z",
     "iopub.status.idle": "2024-01-29T18:45:31.791573Z",
     "shell.execute_reply": "2024-01-29T18:45:31.790589Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 39.55363,
     "end_time": "2024-01-29T18:45:31.793904",
     "exception": false,
     "start_time": "2024-01-29T18:44:52.240274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlorenzozanolin-52\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240129_184501-lfgnfl5a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeep-snowflake-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lorenzozanolin-52/sentiment_analysis\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lorenzozanolin-52/sentiment_analysis/runs/lfgnfl5a\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lorenzozanolin-52/sentiment_analysis/runs/lfgnfl5a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f036c9a7ac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "# allows to have a progress bar in pandas, useful for long processing operations\n",
    "tqdm.pandas()\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"a\")\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=secret_value_0)\n",
    "wandb.init(project='sentiment_analysis', save_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a82bc",
   "metadata": {
    "papermill": {
     "duration": 0.010263,
     "end_time": "2024-01-29T18:45:31.814465",
     "exception": false,
     "start_time": "2024-01-29T18:45:31.804202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Read the dataset and observe the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f726029a",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:45:31.837958Z",
     "iopub.status.busy": "2024-01-29T18:45:31.837664Z",
     "iopub.status.idle": "2024-01-29T18:45:33.276869Z",
     "shell.execute_reply": "2024-01-29T18:45:33.275910Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.453831,
     "end_time": "2024-01-29T18:45:33.279401",
     "exception": false,
     "start_time": "2024-01-29T18:45:31.825570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395742c9",
   "metadata": {
    "papermill": {
     "duration": 0.010414,
     "end_time": "2024-01-29T18:45:33.303493",
     "exception": false,
     "start_time": "2024-01-29T18:45:33.293079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lucky us, the dataset is well-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fce84a0",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:45:33.327995Z",
     "iopub.status.busy": "2024-01-29T18:45:33.327264Z",
     "iopub.status.idle": "2024-01-29T18:45:33.350706Z",
     "shell.execute_reply": "2024-01-29T18:45:33.349812Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.03838,
     "end_time": "2024-01-29T18:45:33.352703",
     "exception": false,
     "start_time": "2024-01-29T18:45:33.314323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1dbac",
   "metadata": {
    "papermill": {
     "duration": 0.011171,
     "end_time": "2024-01-29T18:45:33.374434",
     "exception": false,
     "start_time": "2024-01-29T18:45:33.363263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Transform the labels to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8ca5c3",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:45:33.396488Z",
     "iopub.status.busy": "2024-01-29T18:45:33.396208Z",
     "iopub.status.idle": "2024-01-29T18:45:33.529766Z",
     "shell.execute_reply": "2024-01-29T18:45:33.528566Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.14784,
     "end_time": "2024-01-29T18:45:33.532478",
     "exception": false,
     "start_time": "2024-01-29T18:45:33.384638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:00<00:00, 423971.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   review sentiment  label\n",
       "0      One of the other reviewers has mentioned that ...  positive      1\n",
       "1      A wonderful little production. <br /><br />The...  positive      1\n",
       "2      I thought this was a wonderful way to spend ti...  positive      1\n",
       "3      Basically there's a family where a little boy ...  negative      0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive      1\n",
       "...                                                  ...       ...    ...\n",
       "49995  I thought this movie did a down right good job...  positive      1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative      0\n",
       "49997  I am a Catholic taught in parochial elementary...  negative      0\n",
       "49998  I'm going to have to disagree with the previou...  negative      0\n",
       "49999  No one expects the Star Trek movies to be high...  negative      0\n",
       "\n",
       "[50000 rows x 3 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_label(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "\n",
    "data['label'] = data['sentiment'].progress_apply(transform_label)\n",
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f421fb",
   "metadata": {
    "papermill": {
     "duration": 0.011433,
     "end_time": "2024-01-29T18:45:33.556960",
     "exception": false,
     "start_time": "2024-01-29T18:45:33.545527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "- In classic NLP, the text is often preprocessed to remove tokens that might confuse the classifier\n",
    "- Below you can find some examples of possible preprocessing techniques\n",
    "- Feel free to modify them to improve the results of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a6a963",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:45:33.581437Z",
     "iopub.status.busy": "2024-01-29T18:45:33.580860Z",
     "iopub.status.idle": "2024-01-29T18:45:34.063717Z",
     "shell.execute_reply": "2024-01-29T18:45:34.062649Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.498536,
     "end_time": "2024-01-29T18:45:34.066828",
     "exception": false,
     "start_time": "2024-01-29T18:45:33.568292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def rm_link(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "\n",
    "# handle case like \"shut up okay?Im only 10 years old\"\n",
    "# become \"shut up okay Im only 10 years old\"\n",
    "def rm_punct2(text):\n",
    "    # return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "\n",
    "\n",
    "def rm_html(text):\n",
    "    # remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove <br /> tags\n",
    "    return re.sub(r'<br />', '', text)\n",
    "\n",
    "\n",
    "def space_bt_punct(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)  # add whitespaces between punctuation\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)  # remove double whitespaces\n",
    "    return s\n",
    "\n",
    "\n",
    "def rm_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def rm_whitespaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "\n",
    "def rm_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "\n",
    "def rm_contractions(text): #removes contracted form and insert normal form, i.e. he's -> he is\n",
    "    text=re.sub(\"isn't\",'is not',text)\n",
    "    text=re.sub(\"he's\",'he is',text)\n",
    "    text=re.sub(\"wasn't\",'was not',text)\n",
    "    text=re.sub(\"there's\",'there is',text)\n",
    "    text=re.sub(\"couldn't\",'could not',text)\n",
    "    text=re.sub(\"won't\",'will not',text)\n",
    "    text=re.sub(\"they're\",'they are',text)\n",
    "    text=re.sub(\"she's\",'she is',text)\n",
    "    text=re.sub(\"There's\",'there is',text)\n",
    "    text=re.sub(\"wouldn't\",'would not',text)\n",
    "    text=re.sub(\"haven't\",'have not',text)\n",
    "    text=re.sub(\"That's\",'That is',text)\n",
    "    text=re.sub(\"you've\",'you have',text)\n",
    "    text=re.sub(\"He's\",'He is',text)\n",
    "    text=re.sub(\"what's\",'what is',text)\n",
    "    text=re.sub(\"weren't\",'were not',text)\n",
    "    text=re.sub(\"we're\",'we are',text)\n",
    "    text=re.sub(\"hasn't\",'has not',text)\n",
    "    text=re.sub(\"you'd\",'you would',text)\n",
    "    text=re.sub(\"shouldn't\",'should not',text)\n",
    "    text=re.sub(\"let's\",'let us',text)\n",
    "    text=re.sub(\"they've\",'they have',text)\n",
    "    text=re.sub(\"You'll\",'You will',text)\n",
    "    text=re.sub(\"i'm\",'i am',text)\n",
    "    text=re.sub(\"we've\",'we have',text)\n",
    "    text=re.sub(\"it's\",'it is',text)\n",
    "    text=re.sub(\"don't\",'do not',text)\n",
    "    text=re.sub(\"thatÂ´s\",'that is',text)\n",
    "    text=re.sub(\"IÂ´m\",'I am',text)\n",
    "    text=re.sub(\"itâ€™s\",'it is',text)\n",
    "    text=re.sub(\"sheÂ´s\",'she is',text)\n",
    "    text=re.sub(\"heâ€™s'\",'he is',text)\n",
    "    text=re.sub('Iâ€™m','I am',text)\n",
    "    text=re.sub('Iâ€™d','I did',text)\n",
    "    text=re.sub(\"heâ€™s'\",'he is',text)\n",
    "    text=re.sub('thereâ€™s','there is',text)\n",
    "    return text\n",
    "\n",
    "def spell_correction(text):\n",
    "    # if too slow: return text\n",
    "    return text\n",
    "    # https://pypi.org/project/pyspellchecker/\n",
    "    spell = 0#SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            candidate = spell.correction(word)\n",
    "            if candidate is not None:\n",
    "                corrected_text.append(candidate)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "def clean_pipetext(text):\n",
    "    text = text.lower()\n",
    "    no_link = rm_link(text)\n",
    "    no_cont = rm_contractions(no_link)\n",
    "    no_html = rm_html(no_cont)\n",
    "    space_punct = space_bt_punct(no_html)\n",
    "    no_punct = rm_punct2(space_punct)\n",
    "    no_number = rm_number(no_punct)\n",
    "    no_whitespaces = rm_whitespaces(no_number)\n",
    "    no_nonasci = rm_nonascii(no_whitespaces)\n",
    "    no_emoji = rm_emoji(no_nonasci)\n",
    "    spell_corrected = spell_correction(no_emoji)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738943b5",
   "metadata": {
    "papermill": {
     "duration": 0.011848,
     "end_time": "2024-01-29T18:45:34.096034",
     "exception": false,
     "start_time": "2024-01-29T18:45:34.084186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's clean the reviews first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13677bf1",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:45:34.121499Z",
     "iopub.status.busy": "2024-01-29T18:45:34.120468Z",
     "iopub.status.idle": "2024-01-29T18:46:02.357104Z",
     "shell.execute_reply": "2024-01-29T18:46:02.355684Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 28.251511,
     "end_time": "2024-01-29T18:46:02.359366",
     "exception": false,
     "start_time": "2024-01-29T18:45:34.107855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:28<00:00, 1771.56it/s]\n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].progress_apply(clean_pipetext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5263aa",
   "metadata": {
    "papermill": {
     "duration": 0.032308,
     "end_time": "2024-01-29T18:46:02.425787",
     "exception": false,
     "start_time": "2024-01-29T18:46:02.393479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now tokenize and remove stopwords (i.e. the, a, an, etc.) and lemmatize the words (i.e. running -> run, better -> good, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37674293",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:46:02.541186Z",
     "iopub.status.busy": "2024-01-29T18:46:02.540763Z",
     "iopub.status.idle": "2024-01-29T18:46:06.025845Z",
     "shell.execute_reply": "2024-01-29T18:46:06.024564Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.568363,
     "end_time": "2024-01-29T18:46:06.028312",
     "exception": false,
     "start_time": "2024-01-29T18:46:02.459949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\r\n",
      "  warn(RuntimeWarning(msg))\r\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\r\n",
      "[nltk_data]   Package wordnet is already up-to-date!\r\n",
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m nltk.downloader wordnet\n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n",
    "\n",
    "# preprocessing\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def rm_stopwords(text):\n",
    "    return [i for i in text if i not in stopwords]\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "    # make sure lemmas does not contains stopwords\n",
    "    return rm_stopwords(lemmas)\n",
    "\n",
    "\n",
    "def preprocess_pipetext(text):\n",
    "    tokens = tokenize(text)\n",
    "    no_stopwords = rm_stopwords(tokens)\n",
    "    lemmas = lemmatize(no_stopwords)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8149e12e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:46:06.105418Z",
     "iopub.status.busy": "2024-01-29T18:46:06.105077Z",
     "iopub.status.idle": "2024-01-29T18:49:37.193300Z",
     "shell.execute_reply": "2024-01-29T18:49:37.192358Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 211.197639,
     "end_time": "2024-01-29T18:49:37.263349",
     "exception": false,
     "start_time": "2024-01-29T18:46:06.065710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [03:31<00:00, 236.89it/s]\n"
     ]
    }
   ],
   "source": [
    "data['review'] = data['review'].progress_apply(preprocess_pipetext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429aa21",
   "metadata": {
    "papermill": {
     "duration": 0.187678,
     "end_time": "2024-01-29T18:49:37.639267",
     "exception": false,
     "start_time": "2024-01-29T18:49:37.451589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9933a54",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:38.014269Z",
     "iopub.status.busy": "2024-01-29T18:49:38.013873Z",
     "iopub.status.idle": "2024-01-29T18:49:38.025061Z",
     "shell.execute_reply": "2024-01-29T18:49:38.024045Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.205652,
     "end_time": "2024-01-29T18:49:38.027134",
     "exception": false,
     "start_time": "2024-01-29T18:49:37.821482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mentioned watching oz episode hoo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production . filming techniqu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  one reviewer mentioned watching oz episode hoo...  positive      1\n",
       "1  wonderful little production . filming techniqu...  positive      1\n",
       "2  thought wonderful way spend time hot summer we...  positive      1\n",
       "3  basically family little boy jake think zombie ...  negative      0\n",
       "4  petter mattei love time money visually stunnin...  positive      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076507e",
   "metadata": {
    "papermill": {
     "duration": 0.184911,
     "end_time": "2024-01-29T18:49:38.399041",
     "exception": false,
     "start_time": "2024-01-29T18:49:38.214130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embedding\n",
    "\n",
    "- ANNs cannot process text input\n",
    "- Input tokens must be mapped to integers using a vocabulary\n",
    "- In this example, we build a vocabulary manually, but you can also replace this code with an [embedding layer](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46263eb0",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:38.783577Z",
     "iopub.status.busy": "2024-01-29T18:49:38.783172Z",
     "iopub.status.idle": "2024-01-29T18:49:40.584056Z",
     "shell.execute_reply": "2024-01-29T18:49:40.582960Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.991788,
     "end_time": "2024-01-29T18:49:40.586187",
     "exception": false,
     "start_time": "2024-01-29T18:49:38.594399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get all processed reviews\n",
    "reviews = data.review.values\n",
    "# merge into single variable, separated by whitespaces\n",
    "words = ' '.join(reviews)\n",
    "# obtain list of words\n",
    "words = words.split()\n",
    "# build vocabulary\n",
    "counter = Counter(words)\n",
    "# only keep top 2000 words\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)[:2000]\n",
    "int2word = dict(enumerate(vocab, 2))\n",
    "int2word[0] = '<PAD>'\n",
    "int2word[1] = '<UNK>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62717d1",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:40.974007Z",
     "iopub.status.busy": "2024-01-29T18:49:40.973330Z",
     "iopub.status.idle": "2024-01-29T18:49:43.175925Z",
     "shell.execute_reply": "2024-01-29T18:49:43.175028Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.39444,
     "end_time": "2024-01-29T18:49:43.178039",
     "exception": false,
     "start_time": "2024-01-29T18:49:40.783599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 22798.08it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews_enc = [[word2int[word] if word in word2int else word2int['<UNK>'] for word in review.split()] for review in tqdm(reviews, desc='encoding')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe99e9c",
   "metadata": {
    "papermill": {
     "duration": 0.190746,
     "end_time": "2024-01-29T18:49:43.563801",
     "exception": false,
     "start_time": "2024-01-29T18:49:43.373055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Because we have to build batch, we have to pad the reviews to the same length. We will pad the reviews with <PAD> token.\n",
    "**Because we use RNNs, we need to left pad and not right pad the sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4480e5",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:43.964796Z",
     "iopub.status.busy": "2024-01-29T18:49:43.964426Z",
     "iopub.status.idle": "2024-01-29T18:49:44.797364Z",
     "shell.execute_reply": "2024-01-29T18:49:44.796533Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.040183,
     "end_time": "2024-01-29T18:49:44.799702",
     "exception": false,
     "start_time": "2024-01-29T18:49:43.759519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# left padding sequences\n",
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    # features = np.zeros((len(reviews), seq_length), dtype=int)\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        start_index = max(0, seq_length - len(row))\n",
    "        # if seq_length < len(row) then review will be trimmed\n",
    "        features[i, start_index:] = np.array(row)[:min(seq_length, len(row))]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "seq_length = 128\n",
    "features = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14710c22",
   "metadata": {
    "papermill": {
     "duration": 0.247902,
     "end_time": "2024-01-29T18:49:45.268021",
     "exception": false,
     "start_time": "2024-01-29T18:49:45.020119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8118d6e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:45.652036Z",
     "iopub.status.busy": "2024-01-29T18:49:45.651653Z",
     "iopub.status.idle": "2024-01-29T18:49:45.717164Z",
     "shell.execute_reply": "2024-01-29T18:49:45.716299Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.261519,
     "end_time": "2024-01-29T18:49:45.719658",
     "exception": false,
     "start_time": "2024-01-29T18:49:45.458139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = data.label.to_numpy()\n",
    "\n",
    "# train test split\n",
    "train_size = .75  # we will use 75% of whole data as train set\n",
    "val_size = .5  # and we will use 50% of test set as validation set\n",
    "\n",
    "# stratify will make sure that train and test set have same distribution of labels\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=1 - train_size, stratify=labels)\n",
    "\n",
    "# split test set into validation and test set\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=val_size, stratify=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912fb58c",
   "metadata": {
    "papermill": {
     "duration": 0.18899,
     "end_time": "2024-01-29T18:49:46.096167",
     "exception": false,
     "start_time": "2024-01-29T18:49:45.907177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56553ce",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:46.479036Z",
     "iopub.status.busy": "2024-01-29T18:49:46.478639Z",
     "iopub.status.idle": "2024-01-29T18:49:46.494126Z",
     "shell.execute_reply": "2024-01-29T18:49:46.493164Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.211729,
     "end_time": "2024-01-29T18:49:46.496268",
     "exception": false,
     "start_time": "2024-01-29T18:49:46.284539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define batch size\n",
    "batch_size = 128\n",
    "\n",
    "wandb.log({'batch_size': batch_size})\n",
    "\n",
    "# create tensor datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4274bb",
   "metadata": {
    "papermill": {
     "duration": 0.18943,
     "end_time": "2024-01-29T18:49:46.881565",
     "exception": false,
     "start_time": "2024-01-29T18:49:46.692135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "584acaeb",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:47.266604Z",
     "iopub.status.busy": "2024-01-29T18:49:47.265737Z",
     "iopub.status.idle": "2024-01-29T18:49:47.273902Z",
     "shell.execute_reply": "2024-01-29T18:49:47.273040Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.201873,
     "end_time": "2024-01-29T18:49:47.276040",
     "exception": false,
     "start_time": "2024-01-29T18:49:47.074167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ManyToOneRNN(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, num_layers, num_classes):\n",
    "        super(ManyToOneRNN, self).__init__()\n",
    "        # Embedding layer to convert input indices to dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, emb_size) #in this case input size is the size of the vocabulary\n",
    "        # set the hidden size and the number of layers for the RNN\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # we will use a RNN with a final Fully connected layer to predict the output (class positive or negative)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True) # (batch, sequence_length, X)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        # weights initialization for the fully connected layer\n",
    "        #nn.init.xavier_normal_(self.fc.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embedding layer to convert input indices to dense vectors\n",
    "        x = self.embedding(x)\n",
    "        # RNN layer will output the hidden state and the output\n",
    "        rnn_out, h_n = self.rnn(x)\n",
    "        # Assuming h_n is a tuple of hidden states from all layers\n",
    "        # Concatenate the hidden states from all layers (assuming the last layer [-1])\n",
    "        h_n = h_n[-1].squeeze(0)\n",
    "        # Pass the concatenated hidden states through the fully connected layer\n",
    "        out = self.fc(h_n)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44f267",
   "metadata": {
    "papermill": {
     "duration": 0.185992,
     "end_time": "2024-01-29T18:49:47.652779",
     "exception": false,
     "start_time": "2024-01-29T18:49:47.466787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47872f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:48.041541Z",
     "iopub.status.busy": "2024-01-29T18:49:48.040830Z",
     "iopub.status.idle": "2024-01-29T18:49:48.530326Z",
     "shell.execute_reply": "2024-01-29T18:49:48.528866Z"
    },
    "papermill": {
     "duration": 0.689547,
     "end_time": "2024-01-29T18:49:48.532770",
     "exception": false,
     "start_time": "2024-01-29T18:49:47.843223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ManyToOneRNN(\n",
      "    (embedding): Embedding(2002, 256)\n",
      "    (rnn): RNN(256, 128, batch_first=True)\n",
      "    (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#wandb.init()\n",
    "emb_size = 256  # Dimension of the word embeddings\n",
    "hidden_size = 128  # Number of features in the hidden state\n",
    "output_size = 1  # Dimension of the output, e.g., for a binary classification problem\n",
    "\n",
    "wandb.log({'Embedding Size': emb_size})\n",
    "\n",
    "model = ManyToOneRNN(input_size=len(word2int),emb_size=emb_size,hidden_size=hidden_size,num_layers=1,num_classes=output_size)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  #we want to move the net on the GPU\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model) # if multiple GPUs use them\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0dd8c",
   "metadata": {
    "papermill": {
     "duration": 0.232901,
     "end_time": "2024-01-29T18:49:48.958813",
     "exception": false,
     "start_time": "2024-01-29T18:49:48.725912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d89fe500",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:49:49.340689Z",
     "iopub.status.busy": "2024-01-29T18:49:49.339906Z",
     "iopub.status.idle": "2024-01-29T18:51:54.058193Z",
     "shell.execute_reply": "2024-01-29T18:51:54.057124Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 124.915765,
     "end_time": "2024-01-29T18:51:54.060375",
     "exception": false,
     "start_time": "2024-01-29T18:49:49.144610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.6923\n",
      "Epoch [1/50], Validation Loss: 0.6879, Validation Accuracy: 0.5453\n",
      "Epoch [2/50], Training Loss: 0.6779\n",
      "Epoch [2/50], Validation Loss: 0.6744, Validation Accuracy: 0.5598\n",
      "Epoch [3/50], Training Loss: 0.6092\n",
      "Epoch [3/50], Validation Loss: 0.5595, Validation Accuracy: 0.7261\n",
      "Epoch [4/50], Training Loss: 0.5417\n",
      "Epoch [4/50], Validation Loss: 0.5589, Validation Accuracy: 0.7246\n",
      "Epoch [5/50], Training Loss: 0.5356\n",
      "Epoch [5/50], Validation Loss: 0.5322, Validation Accuracy: 0.7512\n",
      "Epoch [6/50], Training Loss: 0.4911\n",
      "Epoch [6/50], Validation Loss: 0.5289, Validation Accuracy: 0.7488\n",
      "Epoch [7/50], Training Loss: 0.4775\n",
      "Epoch [7/50], Validation Loss: 0.5033, Validation Accuracy: 0.7683\n",
      "Epoch [8/50], Training Loss: 0.4550\n",
      "Epoch [8/50], Validation Loss: 0.4992, Validation Accuracy: 0.7733\n",
      "Epoch [9/50], Training Loss: 0.4436\n",
      "Epoch [9/50], Validation Loss: 0.4767, Validation Accuracy: 0.7824\n",
      "Epoch [10/50], Training Loss: 0.4445\n",
      "Epoch [10/50], Validation Loss: 0.4954, Validation Accuracy: 0.7704\n",
      "Epoch [11/50], Training Loss: 0.4286\n",
      "Epoch [11/50], Validation Loss: 0.4714, Validation Accuracy: 0.7931\n",
      "Epoch [12/50], Training Loss: 0.4166\n",
      "Epoch [12/50], Validation Loss: 0.4577, Validation Accuracy: 0.7928\n",
      "Epoch [13/50], Training Loss: 0.4016\n",
      "Epoch [13/50], Validation Loss: 0.4492, Validation Accuracy: 0.8008\n",
      "Epoch [14/50], Training Loss: 0.3968\n",
      "Epoch [14/50], Validation Loss: 0.4612, Validation Accuracy: 0.7955\n",
      "Epoch [15/50], Training Loss: 0.3912\n",
      "Epoch [15/50], Validation Loss: 0.4370, Validation Accuracy: 0.8128\n",
      "Epoch [16/50], Training Loss: 0.3762\n",
      "Epoch [16/50], Validation Loss: 0.4285, Validation Accuracy: 0.8154\n",
      "Epoch [17/50], Training Loss: 0.3756\n",
      "Epoch [17/50], Validation Loss: 0.4279, Validation Accuracy: 0.8147\n",
      "Epoch [18/50], Training Loss: 0.3648\n",
      "Epoch [18/50], Validation Loss: 0.4179, Validation Accuracy: 0.8192\n",
      "Epoch [19/50], Training Loss: 0.3545\n",
      "Epoch [19/50], Validation Loss: 0.4206, Validation Accuracy: 0.8208\n",
      "Epoch [20/50], Training Loss: 0.3543\n",
      "Epoch [20/50], Validation Loss: 0.4160, Validation Accuracy: 0.8203\n",
      "Epoch [21/50], Training Loss: 0.3423\n",
      "Epoch [21/50], Validation Loss: 0.4124, Validation Accuracy: 0.8275\n",
      "Epoch [22/50], Training Loss: 0.3419\n",
      "Epoch [22/50], Validation Loss: 0.4317, Validation Accuracy: 0.8138\n",
      "Epoch [23/50], Training Loss: 0.3330\n",
      "Epoch [23/50], Validation Loss: 0.4095, Validation Accuracy: 0.8309\n",
      "Epoch [24/50], Training Loss: 0.3349\n",
      "Epoch [24/50], Validation Loss: 0.4022, Validation Accuracy: 0.8325\n",
      "Epoch [25/50], Training Loss: 0.3198\n",
      "Epoch [25/50], Validation Loss: 0.4050, Validation Accuracy: 0.8323\n",
      "Epoch [26/50], Training Loss: 0.3161\n",
      "Epoch [26/50], Validation Loss: 0.4204, Validation Accuracy: 0.8288\n",
      "Epoch [27/50], Training Loss: 0.3086\n",
      "Epoch [27/50], Validation Loss: 0.4120, Validation Accuracy: 0.8334\n",
      "Epoch [28/50], Training Loss: 0.3164\n",
      "Epoch [28/50], Validation Loss: 0.4373, Validation Accuracy: 0.8128\n",
      "Epoch [29/50], Training Loss: 0.3039\n",
      "Epoch [29/50], Validation Loss: 0.3959, Validation Accuracy: 0.8326\n",
      "Epoch [30/50], Training Loss: 0.2998\n",
      "Epoch [30/50], Validation Loss: 0.3933, Validation Accuracy: 0.8368\n",
      "Epoch [31/50], Training Loss: 0.2945\n",
      "Epoch [31/50], Validation Loss: 0.3972, Validation Accuracy: 0.8354\n",
      "Epoch [32/50], Training Loss: 0.2887\n",
      "Epoch [32/50], Validation Loss: 0.4135, Validation Accuracy: 0.8218\n",
      "Epoch [33/50], Training Loss: 0.2874\n",
      "Epoch [33/50], Validation Loss: 0.4063, Validation Accuracy: 0.8336\n",
      "Epoch [34/50], Training Loss: 0.2761\n",
      "Epoch [34/50], Validation Loss: 0.4111, Validation Accuracy: 0.8282\n",
      "Epoch [35/50], Training Loss: 0.2754\n",
      "Epoch [35/50], Validation Loss: 0.4045, Validation Accuracy: 0.8382\n",
      "Epoch [36/50], Training Loss: 0.2706\n",
      "Epoch [36/50], Validation Loss: 0.4462, Validation Accuracy: 0.8178\n",
      "Epoch [37/50], Training Loss: 0.2714\n",
      "Epoch [37/50], Validation Loss: 0.4071, Validation Accuracy: 0.8382\n",
      "Epoch [38/50], Training Loss: 0.2629\n",
      "Epoch [38/50], Validation Loss: 0.4045, Validation Accuracy: 0.8370\n",
      "Epoch [39/50], Training Loss: 0.2616\n",
      "Epoch [39/50], Validation Loss: 0.4366, Validation Accuracy: 0.8251\n",
      "Epoch [40/50], Training Loss: 0.2595\n",
      "Epoch [40/50], Validation Loss: 0.4023, Validation Accuracy: 0.8371\n",
      "Epoch [41/50], Training Loss: 0.2563\n",
      "Epoch [41/50], Validation Loss: 0.4159, Validation Accuracy: 0.8298\n",
      "Epoch [42/50], Training Loss: 0.2451\n",
      "Epoch [42/50], Validation Loss: 0.4061, Validation Accuracy: 0.8381\n",
      "Epoch [43/50], Training Loss: 0.2465\n",
      "Epoch [43/50], Validation Loss: 0.4156, Validation Accuracy: 0.8360\n",
      "Epoch [44/50], Training Loss: 0.2445\n",
      "Epoch [44/50], Validation Loss: 0.4088, Validation Accuracy: 0.8354\n",
      "Epoch [45/50], Training Loss: 0.2376\n",
      "Epoch [45/50], Validation Loss: 0.4102, Validation Accuracy: 0.8371\n",
      "Epoch [46/50], Training Loss: 0.2390\n",
      "Epoch [46/50], Validation Loss: 0.4097, Validation Accuracy: 0.8358\n",
      "Epoch [47/50], Training Loss: 0.2286\n",
      "Epoch [47/50], Validation Loss: 0.4409, Validation Accuracy: 0.8320\n",
      "Epoch [48/50], Training Loss: 0.2235\n",
      "Epoch [48/50], Validation Loss: 0.4261, Validation Accuracy: 0.8344\n",
      "Epoch [49/50], Training Loss: 0.2269\n",
      "Epoch [49/50], Validation Loss: 0.4894, Validation Accuracy: 0.8054\n",
      "Epoch [50/50], Training Loss: 0.2219\n",
      "Epoch [50/50], Validation Loss: 0.4283, Validation Accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "o = 'a'\n",
    "\n",
    "if o == 'a':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    wandb.log({'optimizer':'Adam'})\n",
    "elif o == 'r':\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, alpha=0.9)\n",
    "    wandb.log({'optimizer':'RMSprop'})\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "wandb.log({'lr': lr})\n",
    "wandb.log({'epochs': num_epochs})\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Variable for total loss in each epoch\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Iterate through the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)  #move data on the GPU\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Reshape the labels\n",
    "        labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n",
    "            \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.float())  # Convert labels to float\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the total loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average loss per epoch\n",
    "    Training_Loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Print the average loss per epoch during training\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {Training_Loss:.4f}')\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Variables for total loss and number of correct predictions\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Iterate through the validation data\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)  #move data on the GPU\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Reshape the labels\n",
    "            labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n",
    "                \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels.float())  # Convert labels to float\n",
    "            \n",
    "            # Update the total loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate the number of correct predictions\n",
    "            threshold = 0.5\n",
    "            predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n",
    "            correct_predictions += (predicted_labels == labels.float()).sum().item()\n",
    "\n",
    "    \n",
    "    # Calculate the average loss per epoch during evaluation\n",
    "    average_loss = total_loss / len(valid_loader)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / len(valid_loader.dataset)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Log metrics using WandB\n",
    "    wandb.log({\"Epoch\": epoch+1,\"Training Loss\": Training_Loss, \"Validation Loss\": average_loss, \"Validation Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fab162",
   "metadata": {
    "papermill": {
     "duration": 0.190108,
     "end_time": "2024-01-29T18:51:54.441645",
     "exception": false,
     "start_time": "2024-01-29T18:51:54.251537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5014ab0e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-29T18:51:54.829896Z",
     "iopub.status.busy": "2024-01-29T18:51:54.829143Z",
     "iopub.status.idle": "2024-01-29T18:51:59.447918Z",
     "shell.execute_reply": "2024-01-29T18:51:59.447105Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.816951,
     "end_time": "2024-01-29T18:51:59.450238",
     "exception": false,
     "start_time": "2024-01-29T18:51:54.633287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4079, Test Accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      Embedding Size â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               Epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Test Accuracy â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           Test Loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Training Loss â–ˆâ–ˆâ–‡â–†â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Validation Accuracy â–â–â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     Validation Loss â–ˆâ–ˆâ–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          batch_size â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              epochs â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      Embedding Size 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               Epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Test Accuracy 0.84624\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           Test Loss 0.4079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Training Loss 0.22191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Validation Accuracy 0.8368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     Validation Loss 0.42825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          batch_size 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              epochs 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           optimizer Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mdeep-snowflake-13\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/lorenzozanolin-52/sentiment_analysis/runs/lfgnfl5a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240129_184501-lfgnfl5a/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Variables for total loss and number of correct predictions\n",
    "total_loss = 0.0\n",
    "correct_predictions = 0\n",
    "\n",
    "# Iterate through the test data\n",
    "with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)  #move data on the GPU\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Reshape the model's output\n",
    "        outputs = outputs.view(-1)  # Change dimensions from [batch_size, 1] to [batch_size]\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels.float())  # Convert labels to float\n",
    "        \n",
    "        # Update the total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate the number of correct predictions\n",
    "        threshold = 0.5\n",
    "        predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n",
    "        correct_predictions += (predicted_labels == labels.float()).sum().item()\n",
    "        \n",
    "# Calculate the average loss for the test set\n",
    "average_loss = total_loss / len(test_loader)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = correct_predictions / len(test_loader.dataset)\n",
    "\n",
    "print(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Log metrics using WandB\n",
    "wandb.log({\"Test Loss\": average_loss, \"Test Accuracy\": accuracy})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 434.000133,
   "end_time": "2024-01-29T18:52:02.367610",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-29T18:44:48.367477",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
