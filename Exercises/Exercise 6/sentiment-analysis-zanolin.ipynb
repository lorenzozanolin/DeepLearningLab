{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMDB sentiment analysis with RNNs\n\nKaggle: https://www.kaggle.com/code/lorenzozanolin/sentiment-analysis-zanolin?scriptVersionId=160912821\n\nW&B:https://wandb.ai/lorenzozanolin-52/sentiment_analysis/table?workspace=user-lorenzozanolin-52","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\n#from spellchecker import SpellChecker\nfrom tqdm import tqdm\n# allows to have a progress bar in pandas, useful for long processing operations\ntqdm.pandas()\nfrom collections import Counter\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"a\")\n\nimport wandb\nwandb.login(key=secret_value_0)\nwandb.init(project='sentiment_analysis', save_code=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the dataset and observe the first 5 rows.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndata.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lucky us, the dataset is well-balanced.","metadata":{}},{"cell_type":"code","source":"data.sentiment.value_counts()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transform the labels to 0 and 1.","metadata":{}},{"cell_type":"code","source":"def transform_label(label):\n    return 1 if label == 'positive' else 0\n\n\ndata['label'] = data['sentiment'].progress_apply(transform_label)\ndata.head","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n\n- In classic NLP, the text is often preprocessed to remove tokens that might confuse the classifier\n- Below you can find some examples of possible preprocessing techniques\n- Feel free to modify them to improve the results of your classifier","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('omw-1.4')\nstopwords = set(stopwords.words('english'))\n\ndef rm_link(text):\n    return re.sub(r'http\\S+', '', text)\n\n\n# handle case like \"shut up okay?Im only 10 years old\"\n# become \"shut up okay Im only 10 years old\"\ndef rm_punct2(text):\n    # return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n\n\ndef rm_html(text):\n    # remove html tags\n    text = re.sub(r'<.*?>', '', text)\n    # remove <br /> tags\n    return re.sub(r'<br />', '', text)\n\n\ndef space_bt_punct(text):\n    pattern = r'([.,!?-])'\n    s = re.sub(pattern, r' \\1 ', text)  # add whitespaces between punctuation\n    s = re.sub(r'\\s{2,}', ' ', s)  # remove double whitespaces\n    return s\n\n\ndef rm_number(text):\n    return re.sub(r'\\d+', '', text)\n\n\ndef rm_whitespaces(text):\n    return re.sub(r'\\s+', ' ', text)\n\n\ndef rm_nonascii(text):\n    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n\n\ndef rm_emoji(text):\n    emojis = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE\n    )\n    return emojis.sub(r'', text)\n\ndef rm_contractions(text): #removes contracted form and insert normal form, i.e. he's -> he is\n    text=re.sub(\"isn't\",'is not',text)\n    text=re.sub(\"he's\",'he is',text)\n    text=re.sub(\"wasn't\",'was not',text)\n    text=re.sub(\"there's\",'there is',text)\n    text=re.sub(\"couldn't\",'could not',text)\n    text=re.sub(\"won't\",'will not',text)\n    text=re.sub(\"they're\",'they are',text)\n    text=re.sub(\"she's\",'she is',text)\n    text=re.sub(\"There's\",'there is',text)\n    text=re.sub(\"wouldn't\",'would not',text)\n    text=re.sub(\"haven't\",'have not',text)\n    text=re.sub(\"That's\",'That is',text)\n    text=re.sub(\"you've\",'you have',text)\n    text=re.sub(\"He's\",'He is',text)\n    text=re.sub(\"what's\",'what is',text)\n    text=re.sub(\"weren't\",'were not',text)\n    text=re.sub(\"we're\",'we are',text)\n    text=re.sub(\"hasn't\",'has not',text)\n    text=re.sub(\"you'd\",'you would',text)\n    text=re.sub(\"shouldn't\",'should not',text)\n    text=re.sub(\"let's\",'let us',text)\n    text=re.sub(\"they've\",'they have',text)\n    text=re.sub(\"You'll\",'You will',text)\n    text=re.sub(\"i'm\",'i am',text)\n    text=re.sub(\"we've\",'we have',text)\n    text=re.sub(\"it's\",'it is',text)\n    text=re.sub(\"don't\",'do not',text)\n    text=re.sub(\"that´s\",'that is',text)\n    text=re.sub(\"I´m\",'I am',text)\n    text=re.sub(\"it’s\",'it is',text)\n    text=re.sub(\"she´s\",'she is',text)\n    text=re.sub(\"he’s'\",'he is',text)\n    text=re.sub('I’m','I am',text)\n    text=re.sub('I’d','I did',text)\n    text=re.sub(\"he’s'\",'he is',text)\n    text=re.sub('there’s','there is',text)\n    return text\n\ndef spell_correction(text):\n    # if too slow: return text\n    return text\n    # https://pypi.org/project/pyspellchecker/\n    spell = 0#SpellChecker()\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            candidate = spell.correction(word)\n            if candidate is not None:\n                corrected_text.append(candidate)\n            else:\n                corrected_text.append(word)\n        else:\n            corrected_text.append(word)\n    return ' '.join(corrected_text)\n\ndef clean_pipetext(text):\n    text = text.lower()\n    no_link = rm_link(text)\n    no_cont = rm_contractions(no_link)\n    no_html = rm_html(no_cont)\n    space_punct = space_bt_punct(no_html)\n    no_punct = rm_punct2(space_punct)\n    no_number = rm_number(no_punct)\n    no_whitespaces = rm_whitespaces(no_number)\n    no_nonasci = rm_nonascii(no_whitespaces)\n    no_emoji = rm_emoji(no_nonasci)\n    spell_corrected = spell_correction(no_emoji)\n    return spell_corrected","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's clean the reviews first:","metadata":{}},{"cell_type":"code","source":"data['review'] = data['review'].progress_apply(clean_pipetext)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now tokenize and remove stopwords (i.e. the, a, an, etc.) and lemmatize the words (i.e. running -> run, better -> good, etc.).","metadata":{}},{"cell_type":"code","source":"!python3 -m nltk.downloader wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\n# preprocessing\ndef tokenize(text):\n    return word_tokenize(text)\n\n\ndef rm_stopwords(text):\n    return [i for i in text if i not in stopwords]\n\n\ndef lemmatize(text):\n    lemmatizer = WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(t) for t in text]\n    # make sure lemmas does not contains stopwords\n    return rm_stopwords(lemmas)\n\n\ndef preprocess_pipetext(text):\n    tokens = tokenize(text)\n    no_stopwords = rm_stopwords(tokens)\n    lemmas = lemmatize(no_stopwords)\n    return ' '.join(lemmas)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['review'] = data['review'].progress_apply(preprocess_pipetext)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the result.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding\n\n- ANNs cannot process text input\n- Input tokens must be mapped to integers using a vocabulary\n- In this example, we build a vocabulary manually, but you can also replace this code with an [embedding layer](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)","metadata":{}},{"cell_type":"code","source":"# get all processed reviews\nreviews = data.review.values\n# merge into single variable, separated by whitespaces\nwords = ' '.join(reviews)\n# obtain list of words\nwords = words.split()\n# build vocabulary\ncounter = Counter(words)\n# only keep top 2000 words\nvocab = sorted(counter, key=counter.get, reverse=True)[:2000]\nint2word = dict(enumerate(vocab, 2))\nint2word[0] = '<PAD>'\nint2word[1] = '<UNK>'\nword2int = {word: id for id, word in int2word.items()}","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_enc = [[word2int[word] if word in word2int else word2int['<UNK>'] for word in review.split()] for review in tqdm(reviews, desc='encoding')]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we have to build batch, we have to pad the reviews to the same length. We will pad the reviews with <PAD> token.\n**Because we use RNNs, we need to left pad and not right pad the sequence.**","metadata":{}},{"cell_type":"code","source":"# left padding sequences\ndef pad_features(reviews, pad_id, seq_length=128):\n    # features = np.zeros((len(reviews), seq_length), dtype=int)\n    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n\n    for i, row in enumerate(reviews):\n        start_index = max(0, seq_length - len(row))\n        # if seq_length < len(row) then review will be trimmed\n        features[i, start_index:] = np.array(row)[:min(seq_length, len(row))]\n\n    return features\n\n\nseq_length = 128\nfeatures = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the data","metadata":{}},{"cell_type":"code","source":"labels = data.label.to_numpy()\n\n# train test split\ntrain_size = .75  # we will use 75% of whole data as train set\nval_size = .5  # and we will use 50% of test set as validation set\n\n# stratify will make sure that train and test set have same distribution of labels\ntrain_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=1 - train_size, stratify=labels)\n\n# split test set into validation and test set\nval_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=val_size, stratify=test_y)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the datasets and dataloaders.","metadata":{}},{"cell_type":"code","source":"# define batch size\nbatch_size = 128\n\nwandb.log({'batch_size': batch_size})\n\n# create tensor datasets\ntrain_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_dataset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_dataset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# create dataloaders\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the model.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass ManyToOneRNN(nn.Module):\n    def __init__(self, input_size, emb_size, hidden_size, num_layers, num_classes):\n        super(ManyToOneRNN, self).__init__()\n        # Embedding layer to convert input indices to dense vectors\n        self.embedding = nn.Embedding(input_size, emb_size) #in this case input size is the size of the vocabulary\n        # set the hidden size and the number of layers for the RNN\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        # we will use a RNN with a final Fully connected layer to predict the output (class positive or negative)\n        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True) # (batch, sequence_length, X)\n        self.fc = nn.Linear(hidden_size, num_classes)\n        # weights initialization for the fully connected layer\n        #nn.init.xavier_normal_(self.fc.weight)\n    \n    def forward(self, x):\n        # Embedding layer to convert input indices to dense vectors\n        x = self.embedding(x)\n        # RNN layer will output the hidden state and the output\n        rnn_out, h_n = self.rnn(x)\n        # Assuming h_n is a tuple of hidden states from all layers\n        # Concatenate the hidden states from all layers (assuming the last layer [-1])\n        h_n = h_n[-1].squeeze(0)\n        # Pass the concatenated hidden states through the fully connected layer\n        out = self.fc(h_n)\n        \n        return out","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instantiate the model.","metadata":{}},{"cell_type":"code","source":"#wandb.init()\nemb_size = 256  # Dimension of the word embeddings\nhidden_size = 128  # Number of features in the hidden state\noutput_size = 1  # Dimension of the output, e.g., for a binary classification problem\n\nwandb.log({'Embedding Size': emb_size})\n\nmodel = ManyToOneRNN(input_size=len(word2int),emb_size=emb_size,hidden_size=hidden_size,num_layers=1,num_classes=output_size)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'  #we want to move the net on the GPU\nmodel = model.to(device)\nif device == 'cuda':\n    model = torch.nn.DataParallel(model) # if multiple GPUs use them\n    \nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the training loop.","metadata":{}},{"cell_type":"code","source":"lr = 0.001\no = 'a'\n\nif o == 'a':\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001)\n    wandb.log({'optimizer':'Adam'})\nelif o == 'r':\n    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, alpha=0.9)\n    wandb.log({'optimizer':'RMSprop'})\n\ncriterion = nn.BCEWithLogitsLoss()\n\nnum_epochs = 20\n\nwandb.log({'lr': lr})\nwandb.log({'epochs': num_epochs})\n\nfor epoch in range(num_epochs):\n    # Set the model to training mode\n    model.train()\n    # Variable for total loss in each epoch\n    total_loss = 0.0\n    \n    # Iterate through the training data\n    for inputs, labels in train_loader:\n        inputs = inputs.to(device)  #move data on the GPU\n        labels = labels.to(device)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Reshape the labels\n        labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n            \n        # Compute the loss\n        loss = criterion(outputs, labels.float())  # Convert labels to float\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Update the total loss\n        total_loss += loss.item()\n    \n    # Calculate the average loss per epoch\n    Training_Loss = total_loss / len(train_loader)\n    \n    # Print the average loss per epoch during training\n    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {Training_Loss:.4f}')\n    \n    # Set the model to evaluation mode\n    model.eval()\n    \n    # Variables for total loss and number of correct predictions\n    total_loss = 0.0\n    correct_predictions = 0\n\n    # Iterate through the validation data\n    with torch.no_grad():  # Disable gradient computation during evaluation\n        for inputs, labels in valid_loader:\n            inputs = inputs.to(device)  #move data on the GPU\n            labels = labels.to(device)\n            \n            # Forward pass\n            \n            outputs = model(inputs)\n            \n            # Reshape the labels\n            labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n                \n            # Compute the loss\n            loss = criterion(outputs, labels.float())  # Convert labels to float\n            \n            # Update the total loss\n            total_loss += loss.item()\n            \n            # Calculate the number of correct predictions\n            threshold = 0.5\n            predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n            correct_predictions += (predicted_labels == labels.float()).sum().item()\n\n    \n    # Calculate the average loss per epoch during evaluation\n    average_loss = total_loss / len(valid_loader)\n    \n    # Calculate accuracy\n    accuracy = correct_predictions / len(valid_loader.dataset)\n    \n    # Print evaluation metrics\n    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n    \n    # Log metrics using WandB\n    wandb.log({\"Epoch\": epoch+1,\"Training Loss\": Training_Loss, \"Validation Loss\": average_loss, \"Validation Accuracy\": accuracy})","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Variables for total loss and number of correct predictions\ntotal_loss = 0.0\ncorrect_predictions = 0\n\n# Iterate through the test data\nwith torch.no_grad():  # Disable gradient computation during evaluation\n    for inputs, labels in test_loader:\n        inputs = inputs.to(device)  #move data on the GPU\n        labels = labels.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Reshape the model's output\n        outputs = outputs.view(-1)  # Change dimensions from [batch_size, 1] to [batch_size]\n        \n        # Compute the loss\n        loss = criterion(outputs, labels.float())  # Convert labels to float\n        \n        # Update the total loss\n        total_loss += loss.item()\n        \n        # Calculate the number of correct predictions\n        threshold = 0.5\n        predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n        correct_predictions += (predicted_labels == labels.float()).sum().item()\n        \n# Calculate the average loss for the test set\naverage_loss = total_loss / len(test_loader)\n\n# Calculate accuracy on the test set\naccuracy = correct_predictions / len(test_loader.dataset)\n\nprint(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n\n# Log metrics using WandB\nwandb.log({\"Test Loss\": average_loss, \"Test Accuracy\": accuracy})\nwandb.finish()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}