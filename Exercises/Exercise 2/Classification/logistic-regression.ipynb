{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic - Machine Learning from Disaster\n\n\nKaggle link: https://www.kaggle.com/c/titanic","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install wandb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):    # ''\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-11-07T16:08:23.400629Z","iopub.execute_input":"2023-11-07T16:08:23.401241Z","iopub.status.idle":"2023-11-07T16:08:40.281167Z","shell.execute_reply.started":"2023-11-07T16:08:23.401202Z","shell.execute_reply":"2023-11-07T16:08:40.279818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import all the needed library and init Weights and Biases","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nimport pandas as pd\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"a\")\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:40.283857Z","iopub.execute_input":"2023-11-07T16:08:40.284547Z","iopub.status.idle":"2023-11-07T16:08:49.099080Z","shell.execute_reply.started":"2023-11-07T16:08:40.284495Z","shell.execute_reply":"2023-11-07T16:08:49.097730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first need to read the datasets","metadata":{}},{"cell_type":"code","source":"titanic_training_data = pd.read_csv('/kaggle/input/titanic/train.csv')    #/kaggle/input/titanic/train.csv './titanic/train.csv'\ntitanic_test_data = pd.read_csv('/kaggle/input/titanic/test.csv')\ntitanic_training_data.shape\ntitanic_training_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.101178Z","iopub.execute_input":"2023-11-07T16:08:49.102231Z","iopub.status.idle":"2023-11-07T16:08:49.162367Z","shell.execute_reply.started":"2023-11-07T16:08:49.102179Z","shell.execute_reply":"2023-11-07T16:08:49.161065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataframe needs to be cleaned, knowing if some informations are unknown can be very important to determine if someone survived","metadata":{}},{"cell_type":"code","source":"def clean_titanic(df, train=True):\n    df[\"Cabin\"] = df[\"Cabin\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"Embarked\"] = df[\"Embarked\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"AgeNan\"] = df[\"Age\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df = pd.concat([df, pd.get_dummies(df['Sex'], dtype='bool', prefix='sex_'), pd.get_dummies(df['Pclass'], dtype='bool', prefix='pclass_')], axis=1)\n    df = df.drop(['PassengerId', 'Name','Ticket','Sex','Pclass'], axis=1)\n    if train:\n        df = df.drop(['Survived'], axis=1)\n    numeric_features = df.dtypes[(df.dtypes != 'object') & (df.dtypes != 'bool')].index\n    df[numeric_features] = df[numeric_features].apply(lambda x: (x - x.mean()) / (x.std())) #mean normalization\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    return df\n\ny_data = torch.tensor(titanic_training_data[\"Survived\"].values, dtype=torch.float32)\nX_data = clean_titanic(titanic_training_data)\nX_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.165694Z","iopub.execute_input":"2023-11-07T16:08:49.166213Z","iopub.status.idle":"2023-11-07T16:08:49.273496Z","shell.execute_reply.started":"2023-11-07T16:08:49.166168Z","shell.execute_reply":"2023-11-07T16:08:49.272353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then transform the data from numpy (pandas representation) into torch's `Tensor`","metadata":{}},{"cell_type":"code","source":"X_data = torch.tensor(X_data.astype('float').values, dtype=torch.float32)\nX_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.275138Z","iopub.execute_input":"2023-11-07T16:08:49.275475Z","iopub.status.idle":"2023-11-07T16:08:49.284278Z","shell.execute_reply.started":"2023-11-07T16:08:49.275447Z","shell.execute_reply":"2023-11-07T16:08:49.283031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a `TensorDataset` to get tuple of data and label","metadata":{}},{"cell_type":"code","source":"dataset = torch.utils.data.TensorDataset(X_data, y_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.285775Z","iopub.execute_input":"2023-11-07T16:08:49.286158Z","iopub.status.idle":"2023-11-07T16:08:49.293848Z","shell.execute_reply.started":"2023-11-07T16:08:49.286127Z","shell.execute_reply":"2023-11-07T16:08:49.292949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then split between the training and validation set","metadata":{}},{"cell_type":"code","source":"training_size = int(0.7 * len(dataset))\nvalidation_size = len(dataset) - training_size\ntrain, val = torch.utils.data.random_split(dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(0))\ndata_loader_train = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True) #prima era a 32\ndata_loader_val = torch.utils.data.DataLoader(val, batch_size=64, shuffle=True) #prima era a 10","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.295753Z","iopub.execute_input":"2023-11-07T16:08:49.296254Z","iopub.status.idle":"2023-11-07T16:08:49.319715Z","shell.execute_reply.started":"2023-11-07T16:08:49.296215Z","shell.execute_reply":"2023-11-07T16:08:49.318570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Layer initialization using Xavier Uniform on the weight and a constant 0 value on the bias","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F    \n\ndef init_my_layer(m, gain=1):\n    torch.nn.init.xavier_normal_(m.weight, gain)\n    torch.nn.init.constant_(m.bias, 0)\n    return m\n\nclass MyNetwork(nn.Module):\n    def __init__(self):\n        super(MyNetwork, self).__init__() \n        #self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        self.ln = init_my_layer(nn.Linear(12, 1), nn.init.calculate_gain('sigmoid'))\n        #self.ln1 = init_my_layer(nn.Linear(12, 5), nn.init.calculate_gain('tanh'))\n        #self.ln2 = init_my_layer(nn.Linear(5, 1), nn.init.calculate_gain('sigmoid'))\n        \n    def forward(self, x):\n        #x = self.tanh(self.ln1(x))\n        #x = self.sigmoid(self.ln2(x))\n        x = self.sigmoid(self.ln(x))\n        #return F.sigmoid(x) \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.321118Z","iopub.execute_input":"2023-11-07T16:08:49.322087Z","iopub.status.idle":"2023-11-07T16:08:49.331490Z","shell.execute_reply.started":"2023-11-07T16:08:49.322053Z","shell.execute_reply":"2023-11-07T16:08:49.329876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the LinearModel with one Linear layer and Sigmoid applied to the output","metadata":{}},{"cell_type":"code","source":"#TODO Xavier Uniform to the weight and set the bias to 0\nnet = MyNetwork() \nprint(list(net.parameters()))","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.332953Z","iopub.execute_input":"2023-11-07T16:08:49.333371Z","iopub.status.idle":"2023-11-07T16:08:49.417817Z","shell.execute_reply.started":"2023-11-07T16:08:49.333339Z","shell.execute_reply":"2023-11-07T16:08:49.416800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize the network (call it `net`, it would makes things easier later), the loss, the optimizer and write the training loop\n\nDon't forget to check the validation loss and save your model at the end of each epoch!","metadata":{}},{"cell_type":"code","source":"from torch.autograd import Variable\nnum_epochs = 400 \nlr = 3e-3 \nwandb.init(project=\"logistic_regression\",config={\"lr\": lr, \"epochs\": num_epochs}) \ncriterion = nn.BCELoss()    #binary cross entropy loss\n\no = 'r'\n\nif o == 's':\n    #optimizer = torch.optim.SGD(net.parameters(), lr)\n    optimizer = torch.optim.SGD(net.parameters(), lr, weight_decay=1e-4)\n    wandb.log({'optimizer':'SGD'})\nelif o == 'sg':\n    #optimizer = torch.optim.SGD(net.parameters(), lr, momentum=0.9)\n    optimizer = torch.optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=1e-4)\n    wandb.log({'optimizer':'SGD-M'})\nelif o == 'r':\n    optimizer = torch.optim.RMSprop(net.parameters(), lr)\n    wandb.log({'optimizer':'RMS'})\nelif o == 'a':\n    optimizer = torch.optim.Adam(net.parameters(), lr)\n    wandb.log({'optimizer':'Adam'})\n    \nfor epoch in range(num_epochs):\n    training_loss = 0\n    #TRAINING LOOP\n    for X,y in data_loader_train:\n        optimizer.zero_grad()\n        y_pred=net(X)\n        loss=criterion(y_pred,y.reshape(-1, 1))\n        training_loss += loss\n        loss.sum().backward()\n        optimizer.step()\n    validation_loss = 0\n    with torch.no_grad():\n        #VALIDATION LOOP\n        for X,y in data_loader_val:\n            y_pred=net(X)\n            loss=criterion(y_pred,y.reshape(-1, 1))\n            validation_loss+=loss\n\n    print({'epoch':(epoch), 'training_loss': (training_loss/64).item(), 'validation_loss': (validation_loss/64).item()})\n    wandb.log({'training loss': (training_loss/64).item()}, step=epoch)\n    wandb.log({'validation loss': (validation_loss/64).item()}, step=epoch)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:08:49.420871Z","iopub.execute_input":"2023-11-07T16:08:49.421353Z","iopub.status.idle":"2023-11-07T16:09:28.059612Z","shell.execute_reply.started":"2023-11-07T16:08:49.421320Z","shell.execute_reply":"2023-11-07T16:09:28.058266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see the accuracy on the predictions, then we will create the submission file.\n\nThis loop computes the prediction on the test dataset and create a submission file\n","metadata":{}},{"cell_type":"code","source":"titanic_test_data_cleaned = clean_titanic(titanic_test_data, train=False)\ntitanic_data_tensor = torch.tensor(titanic_test_data_cleaned.astype('float').values, dtype=torch.float32)\n\ntest = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n\nwith torch.no_grad():\n    net.eval()\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(titanic_data_tensor):\n        output = net(data)\n        predicted = torch.ge(output, 0.5)\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    out_df = pd.DataFrame(np.c_[titanic_test_data['PassengerId'].values, test_pred.numpy()], columns=['PassengerId', 'Survived'])\n    out_df.to_csv('submission.csv', index=False)\n    \nfrom sklearn.metrics import  accuracy_score\naccuracy = accuracy_score(out_df['Survived'],test['Survived'])\nprint(accuracy)\nwandb.log({'accuracy':accuracy})","metadata":{"execution":{"iopub.status.busy":"2023-11-07T16:09:28.061367Z","iopub.execute_input":"2023-11-07T16:09:28.061740Z","iopub.status.idle":"2023-11-07T16:09:28.335489Z","shell.execute_reply.started":"2023-11-07T16:09:28.061708Z","shell.execute_reply":"2023-11-07T16:09:28.334187Z"},"trusted":true},"execution_count":null,"outputs":[]}]}