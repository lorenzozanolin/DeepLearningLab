{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic - Machine Learning from Disaster\n\n\nKaggle link: https://www.kaggle.com/c/titanic\n\nW&B link: https://wandb.ai/lorenzozanolin-52/logistic_regression/table?workspace=user-lorenzozanolin-52","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n#!pip install wandb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):    # ''\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-11-08T16:00:48.072344Z","iopub.execute_input":"2023-11-08T16:00:48.072775Z","iopub.status.idle":"2023-11-08T16:00:48.085769Z","shell.execute_reply.started":"2023-11-08T16:00:48.072742Z","shell.execute_reply":"2023-11-08T16:00:48.084362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import all the needed library and init Weights and Biases","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nimport pandas as pd\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"a\")\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.087851Z","iopub.execute_input":"2023-11-08T16:00:48.088317Z","iopub.status.idle":"2023-11-08T16:00:48.330156Z","shell.execute_reply.started":"2023-11-08T16:00:48.088284Z","shell.execute_reply":"2023-11-08T16:00:48.328967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first need to read the datasets","metadata":{}},{"cell_type":"code","source":"titanic_training_data = pd.read_csv('/kaggle/input/titanic/train.csv')    #/kaggle/input/titanic/train.csv './titanic/train.csv'\ntitanic_test_data = pd.read_csv('/kaggle/input/titanic/test.csv')\ntitanic_training_data.shape\ntitanic_training_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.331623Z","iopub.execute_input":"2023-11-08T16:00:48.331995Z","iopub.status.idle":"2023-11-08T16:00:48.362307Z","shell.execute_reply.started":"2023-11-08T16:00:48.331952Z","shell.execute_reply":"2023-11-08T16:00:48.361081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataframe needs to be cleaned, knowing if some informations are unknown can be very important to determine if someone survived","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor  \ndef remove_multicollinearity(df,features):   #we will use it to remove features whom VIF is higher than 20\n    vif_data=pd.DataFrame()\n    vif_data[\"Feature\"]=features # names of the features\n    vif_data[\"VIF\"]=[variance_inflation_factor(df[features].values,i) for i in range(len(features))] # VIF score for each features, higher VIF means higher correlation\n    return vif_data.sort_values(by=[\"VIF\"]).reset_index(drop=True)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.366583Z","iopub.execute_input":"2023-11-08T16:00:48.367107Z","iopub.status.idle":"2023-11-08T16:00:48.375173Z","shell.execute_reply.started":"2023-11-08T16:00:48.367058Z","shell.execute_reply":"2023-11-08T16:00:48.373765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_titanic(df, train=True):\n    df[\"Cabin\"] = df[\"Cabin\"].apply(lambda x: pd.isna(x)).astype(bool)  # will set True for each missing Cabin value, False for each cabin whom value was known\n    df[\"Embarked\"] = df[\"Embarked\"].apply(lambda x: pd.isna(x)).astype(bool) # same as before\n    df[\"AgeNan\"] = df[\"Age\"].apply(lambda x: pd.isna(x)).astype(bool) # same as before\n    df = pd.concat([df, pd.get_dummies(df['Sex'], dtype='bool', prefix='sex_'), pd.get_dummies(df['Pclass'], dtype='bool', prefix='pclass_')], axis=1) # adds new columns to the pre-existing dataframe. pd.get_dummies() encodes categorical variables into one-hot encoded dummy variables\n    df = df.drop(['PassengerId', 'Name','Ticket','Sex','Pclass'], axis=1) # removes useless features\n    if train:\n        df = df.drop(['Survived'], axis=1) # removes last column since we are considering the training set\n    numeric_features = df.dtypes[(df.dtypes != 'object') & (df.dtypes != 'bool')].index # This results in a list of column names corresponding to the numeric features\n    df[numeric_features] = df[numeric_features].apply(lambda x: (x - x.mean()) / (x.std())) #mean normalization\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean()) # fills empty values with the mean\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean()) # same\n    \n    return df\n\ny_data = torch.tensor(titanic_training_data[\"Survived\"].values, dtype=torch.float32)\nX_data = clean_titanic(titanic_training_data)\nX_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.377163Z","iopub.execute_input":"2023-11-08T16:00:48.377964Z","iopub.status.idle":"2023-11-08T16:00:48.427170Z","shell.execute_reply.started":"2023-11-08T16:00:48.377905Z","shell.execute_reply":"2023-11-08T16:00:48.425957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then transform the data from numpy (pandas representation) into torch's `Tensor`","metadata":{}},{"cell_type":"code","source":"X_data = torch.tensor(X_data.astype('float').values, dtype=torch.float32)    # create a tensor where each value is a FLOAT\nX_data\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.437198Z","iopub.execute_input":"2023-11-08T16:00:48.437710Z","iopub.status.idle":"2023-11-08T16:00:48.457759Z","shell.execute_reply.started":"2023-11-08T16:00:48.437662Z","shell.execute_reply":"2023-11-08T16:00:48.456356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a `TensorDataset` to get tuple of data and label","metadata":{}},{"cell_type":"code","source":"dataset = torch.utils.data.TensorDataset(X_data, y_data)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.459474Z","iopub.execute_input":"2023-11-08T16:00:48.459888Z","iopub.status.idle":"2023-11-08T16:00:48.473087Z","shell.execute_reply.started":"2023-11-08T16:00:48.459854Z","shell.execute_reply":"2023-11-08T16:00:48.471514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then split between the training and validation set","metadata":{}},{"cell_type":"code","source":"training_size = int(0.7 * len(dataset))\nvalidation_size = len(dataset) - training_size\ntrain, val = torch.utils.data.random_split(dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(0))\ndata_loader_train = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True) #prima era a 32\ndata_loader_val = torch.utils.data.DataLoader(val, batch_size=10, shuffle=True) #prima era a 10","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.474954Z","iopub.execute_input":"2023-11-08T16:00:48.476211Z","iopub.status.idle":"2023-11-08T16:00:48.490799Z","shell.execute_reply.started":"2023-11-08T16:00:48.476162Z","shell.execute_reply":"2023-11-08T16:00:48.489702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Layer initialization using Xavier Uniform on the weight and a constant 0 value on the bias","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F    \n\ndef init_my_layer(m, gain=1):\n    torch.nn.init.xavier_normal_(m.weight, gain)\n    torch.nn.init.constant_(m.bias, 0)\n    return m\n\nclass MyNetwork(nn.Module):\n    def __init__(self):\n        super(MyNetwork, self).__init__() \n        #self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        self.ln = init_my_layer(nn.Linear(12, 1), nn.init.calculate_gain('sigmoid'))\n        #self.ln1 = init_my_layer(nn.Linear(12, 5), nn.init.calculate_gain('tanh'))\n        #self.ln2 = init_my_layer(nn.Linear(5, 1), nn.init.calculate_gain('sigmoid'))\n        \n    def forward(self, x):\n        #x = self.tanh(self.ln1(x))\n        #x = self.sigmoid(self.ln2(x))\n        x = self.sigmoid(self.ln(x))\n        #return F.sigmoid(x) \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.494919Z","iopub.execute_input":"2023-11-08T16:00:48.495703Z","iopub.status.idle":"2023-11-08T16:00:48.504730Z","shell.execute_reply.started":"2023-11-08T16:00:48.495657Z","shell.execute_reply":"2023-11-08T16:00:48.503848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the LinearModel with one Linear layer and Sigmoid applied to the output","metadata":{}},{"cell_type":"code","source":"net = MyNetwork() \nprint(list(net.parameters()))","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.506353Z","iopub.execute_input":"2023-11-08T16:00:48.507102Z","iopub.status.idle":"2023-11-08T16:00:48.525125Z","shell.execute_reply.started":"2023-11-08T16:00:48.507058Z","shell.execute_reply":"2023-11-08T16:00:48.523423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize the network (call it `net`, it would makes things easier later), the loss, the optimizer and write the training loop\n\nDon't forget to check the validation loss and save your model at the end of each epoch!","metadata":{}},{"cell_type":"code","source":"from torch.autograd import Variable\nnum_epochs = 500 \nlr = 3e-3 \nwandb.init(project=\"logistic_regression\",config={\"lr\": lr, \"epochs\": num_epochs}) \ncriterion = nn.BCELoss()    #binary cross entropy loss\n\no = 'r'\n\nif o == 's':\n    #optimizer = torch.optim.SGD(net.parameters(), lr)\n    optimizer = torch.optim.SGD(net.parameters(), lr, weight_decay=1e-4)\n    wandb.log({'optimizer':'SGD'})\nelif o == 'sg':\n    #optimizer = torch.optim.SGD(net.parameters(), lr, momentum=0.9)\n    optimizer = torch.optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=1e-4)\n    wandb.log({'optimizer':'SGD-M'})\nelif o == 'r':\n    optimizer = torch.optim.RMSprop(net.parameters(), lr)\n    wandb.log({'optimizer':'RMS'})\nelif o == 'a':\n    optimizer = torch.optim.Adam(net.parameters(), lr)\n    wandb.log({'optimizer':'Adam'})\n    \nfor epoch in range(num_epochs):\n    training_loss = 0\n    #TRAINING LOOP\n    for X,y in data_loader_train:\n        optimizer.zero_grad()\n        y_pred=net(X)\n        loss=criterion(y_pred,y.reshape(-1, 1))\n        training_loss += loss\n        loss.sum().backward()\n        optimizer.step()\n    validation_loss = 0\n    with torch.no_grad():\n        #VALIDATION LOOP\n        for X,y in data_loader_val:\n            y_pred=net(X)\n            loss=criterion(y_pred,y.reshape(-1, 1))\n            validation_loss+=loss\n\n    print({'epoch':(epoch), 'training_loss': (training_loss/32).item(), 'validation_loss': (validation_loss/10).item()})\n    wandb.log({'training loss': (training_loss/32).item()}, step=epoch)\n    wandb.log({'validation loss': (validation_loss/10).item()}, step=epoch)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:00:48.528274Z","iopub.execute_input":"2023-11-08T16:00:48.528779Z","iopub.status.idle":"2023-11-08T16:10:13.866650Z","shell.execute_reply.started":"2023-11-08T16:00:48.528733Z","shell.execute_reply":"2023-11-08T16:10:13.865667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see the accuracy on the predictions, then we will create the submission file.\n\nThis loop computes the prediction on the test dataset and create a submission file\n","metadata":{}},{"cell_type":"code","source":"titanic_test_data_cleaned = clean_titanic(titanic_test_data, train=False)\ntitanic_data_tensor = torch.tensor(titanic_test_data_cleaned.astype('float').values, dtype=torch.float32)\n\ntest = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n\nwith torch.no_grad():\n    net.eval()\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(titanic_data_tensor):\n        output = net(data)\n        predicted = torch.ge(output, 0.5)\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    out_df = pd.DataFrame(np.c_[titanic_test_data['PassengerId'].values, test_pred.numpy()], columns=['PassengerId', 'Survived'])\n    out_df.to_csv('submission.csv', index=False)\n    \nfrom sklearn.metrics import  accuracy_score\naccuracy = accuracy_score(out_df['Survived'],test['Survived'])\nprint(\"Accuracy over females:\", accuracy)\nwandb.log({'accuracy':accuracy})","metadata":{"execution":{"iopub.status.busy":"2023-11-08T16:10:13.868226Z","iopub.execute_input":"2023-11-08T16:10:13.868930Z","iopub.status.idle":"2023-11-08T16:10:13.940700Z","shell.execute_reply.started":"2023-11-08T16:10:13.868880Z","shell.execute_reply":"2023-11-08T16:10:13.939549Z"},"trusted":true},"execution_count":null,"outputs":[]}]}